{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67j8odKJJQL_"
   },
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qd88JeQfrEzc",
    "outputId": "de341305-e240-4805-c774-1932e21f67c7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Foresight import *\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0N6DJJ0mbk_"
   },
   "source": [
    "### Insuring that training is done on GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2QpZ1E8emblA",
    "outputId": "9d1db2b9-7317-446f-af0b-e5fa5c626d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Parallel Available\")\n",
    "        parallel=True\n",
    "        \n",
    "    else:\n",
    "        parallel=False\n",
    "        \n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "    parallel=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Um9Dkbc5mblC"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weVUDM7lmblD"
   },
   "outputs": [],
   "source": [
    "path = \"../Data/\"\n",
    "universe_dict = universe_select(path, \"MTL\")\n",
    "use_lg_returns = False\n",
    "auto_regressive = False\n",
    "use_PCA = True\n",
    "saving = False\n",
    "loading = False\n",
    "\n",
    "if auto_regressive: assert (auto_regressive!=use_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k8FIHASpHfv"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ki2dJCGNkOVE",
    "outputId": "5dcd0e8a-a29f-48db-caa3-71199e813a92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included Instrument:\n",
      "al_shfe\n",
      "al_lme\n",
      "al_comex_p\n",
      "Rows removed: 1\n",
      "al_comex_s\n",
      "al_lme_s\n",
      "yuan\n",
      "cu_shfe\n",
      "cu_lme\n",
      "cu_comex_p\n",
      "cu_comex_s\n",
      "peso\n",
      "sol\n",
      "bdi\n",
      "ted\n",
      "vix\n",
      "skew\n",
      "gsci\n",
      "sn_lme\n",
      "pb_lme\n",
      "ni_lme\n"
     ]
    }
   ],
   "source": [
    "# Renaming the columns to price\n",
    "universe_dict = price_rename(universe_dict)\n",
    "\n",
    "# Cleaning the dataset of any erroneous datapoints\n",
    "universe_dict = clean_dict_gen(universe_dict)\n",
    "\n",
    "# Making sure that all the points in the window have consistent length\n",
    "universe_dict = truncate_window_length(universe_dict)\n",
    "\n",
    "# # Generating the dataset\n",
    "if use_lg_returns:\n",
    "    # Lg Returns Only\n",
    "    df_full = generate_dataset(universe_dict, lg_only=True, price_only=False)\n",
    "    target_col = [\"cu_lme\", \"al_lme\", \"sn_lme\", \"pb_lme\", \"ni_lme\"]\n",
    "    \n",
    "else:\n",
    "    # Price Only\n",
    "    df_full = generate_dataset(universe_dict, lg_only=False, price_only=True)\n",
    "    target_col = [\"price_cu_lme\", \"price_al_lme\", \"price_sn_lme\", \"price_pb_lme\", \"price_ni_lme\"]\n",
    "\n",
    "if auto_regressive:\n",
    "    df_full = df_full[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "colab_type": "code",
    "id": "7J6bOnusmblO",
    "outputId": "c11be67d-5775-41e6-ac91-5c1ab85c933e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_al_shfe</th>\n",
       "      <th>price_al_lme</th>\n",
       "      <th>price_al_comex_p</th>\n",
       "      <th>price_al_comex_s</th>\n",
       "      <th>price_al_lme_s</th>\n",
       "      <th>price_yuan</th>\n",
       "      <th>price_cu_shfe</th>\n",
       "      <th>price_cu_lme</th>\n",
       "      <th>price_cu_comex_p</th>\n",
       "      <th>price_cu_comex_s</th>\n",
       "      <th>price_peso</th>\n",
       "      <th>price_sol</th>\n",
       "      <th>price_bdi</th>\n",
       "      <th>price_ted</th>\n",
       "      <th>price_vix</th>\n",
       "      <th>price_skew</th>\n",
       "      <th>price_gsci</th>\n",
       "      <th>price_sn_lme</th>\n",
       "      <th>price_pb_lme</th>\n",
       "      <th>price_ni_lme</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-06-04</th>\n",
       "      <td>13270.0</td>\n",
       "      <td>1812.75</td>\n",
       "      <td>2189.75</td>\n",
       "      <td>13403.0</td>\n",
       "      <td>5170375.0</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>49570.0</td>\n",
       "      <td>6834.0</td>\n",
       "      <td>3.0935</td>\n",
       "      <td>16395.0</td>\n",
       "      <td>552.09</td>\n",
       "      <td>2.789</td>\n",
       "      <td>959.0</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>12.08</td>\n",
       "      <td>121.48</td>\n",
       "      <td>645.9893</td>\n",
       "      <td>23225.0</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>19025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-05</th>\n",
       "      <td>13235.0</td>\n",
       "      <td>1824.25</td>\n",
       "      <td>2201.00</td>\n",
       "      <td>13852.0</td>\n",
       "      <td>5160575.0</td>\n",
       "      <td>6.2548</td>\n",
       "      <td>49520.0</td>\n",
       "      <td>6803.0</td>\n",
       "      <td>3.0910</td>\n",
       "      <td>16440.0</td>\n",
       "      <td>550.66</td>\n",
       "      <td>2.789</td>\n",
       "      <td>977.0</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>11.68</td>\n",
       "      <td>122.21</td>\n",
       "      <td>646.6336</td>\n",
       "      <td>23245.0</td>\n",
       "      <td>2109.0</td>\n",
       "      <td>19030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-06</th>\n",
       "      <td>13280.0</td>\n",
       "      <td>1856.75</td>\n",
       "      <td>2222.25</td>\n",
       "      <td>14276.0</td>\n",
       "      <td>5152975.0</td>\n",
       "      <td>6.2498</td>\n",
       "      <td>49550.0</td>\n",
       "      <td>6696.0</td>\n",
       "      <td>3.0530</td>\n",
       "      <td>16323.0</td>\n",
       "      <td>549.03</td>\n",
       "      <td>2.789</td>\n",
       "      <td>989.0</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>10.73</td>\n",
       "      <td>122.88</td>\n",
       "      <td>646.6281</td>\n",
       "      <td>23195.0</td>\n",
       "      <td>2109.0</td>\n",
       "      <td>18875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-09</th>\n",
       "      <td>13385.0</td>\n",
       "      <td>1886.00</td>\n",
       "      <td>2257.00</td>\n",
       "      <td>14876.0</td>\n",
       "      <td>5146525.0</td>\n",
       "      <td>6.2397</td>\n",
       "      <td>48720.0</td>\n",
       "      <td>6688.5</td>\n",
       "      <td>3.0455</td>\n",
       "      <td>16183.0</td>\n",
       "      <td>549.65</td>\n",
       "      <td>2.783</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>11.15</td>\n",
       "      <td>124.67</td>\n",
       "      <td>651.2814</td>\n",
       "      <td>23295.0</td>\n",
       "      <td>2141.0</td>\n",
       "      <td>18905.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-10</th>\n",
       "      <td>13455.0</td>\n",
       "      <td>1865.50</td>\n",
       "      <td>2250.75</td>\n",
       "      <td>15138.0</td>\n",
       "      <td>5140000.0</td>\n",
       "      <td>6.2240</td>\n",
       "      <td>48420.0</td>\n",
       "      <td>6710.0</td>\n",
       "      <td>3.0550</td>\n",
       "      <td>16320.0</td>\n",
       "      <td>553.49</td>\n",
       "      <td>2.789</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>10.99</td>\n",
       "      <td>125.38</td>\n",
       "      <td>648.0516</td>\n",
       "      <td>22865.0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>18775.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            price_al_shfe  price_al_lme  price_al_comex_p  price_al_comex_s  \\\n",
       "date                                                                          \n",
       "2014-06-04        13270.0       1812.75           2189.75           13403.0   \n",
       "2014-06-05        13235.0       1824.25           2201.00           13852.0   \n",
       "2014-06-06        13280.0       1856.75           2222.25           14276.0   \n",
       "2014-06-09        13385.0       1886.00           2257.00           14876.0   \n",
       "2014-06-10        13455.0       1865.50           2250.75           15138.0   \n",
       "\n",
       "            price_al_lme_s  price_yuan  price_cu_shfe  price_cu_lme  \\\n",
       "date                                                                  \n",
       "2014-06-04       5170375.0      6.2494        49570.0        6834.0   \n",
       "2014-06-05       5160575.0      6.2548        49520.0        6803.0   \n",
       "2014-06-06       5152975.0      6.2498        49550.0        6696.0   \n",
       "2014-06-09       5146525.0      6.2397        48720.0        6688.5   \n",
       "2014-06-10       5140000.0      6.2240        48420.0        6710.0   \n",
       "\n",
       "            price_cu_comex_p  price_cu_comex_s  price_peso  price_sol  \\\n",
       "date                                                                    \n",
       "2014-06-04            3.0935           16395.0      552.09      2.789   \n",
       "2014-06-05            3.0910           16440.0      550.66      2.789   \n",
       "2014-06-06            3.0530           16323.0      549.03      2.789   \n",
       "2014-06-09            3.0455           16183.0      549.65      2.783   \n",
       "2014-06-10            3.0550           16320.0      553.49      2.789   \n",
       "\n",
       "            price_bdi  price_ted  price_vix  price_skew  price_gsci  \\\n",
       "date                                                                  \n",
       "2014-06-04      959.0     0.0355      12.08      121.48    645.9893   \n",
       "2014-06-05      977.0     0.0355      11.68      122.21    646.6336   \n",
       "2014-06-06      989.0     0.0330      10.73      122.88    646.6281   \n",
       "2014-06-09      999.0     0.0355      11.15      124.67    651.2814   \n",
       "2014-06-10     1004.0     0.0406      10.99      125.38    648.0516   \n",
       "\n",
       "            price_sn_lme  price_pb_lme  price_ni_lme  \n",
       "date                                                  \n",
       "2014-06-04       23225.0        2103.0       19025.0  \n",
       "2014-06-05       23245.0        2109.0       19030.0  \n",
       "2014-06-06       23195.0        2109.0       18875.0  \n",
       "2014-06-09       23295.0        2141.0       18905.0  \n",
       "2014-06-10       22865.0        2140.0       18775.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head(5)\n",
    "# df_full.tail(5)\n",
    "\n",
    "# Visualise the plots if desired\n",
    "# visualise_df(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x65OxxWrqv4Q"
   },
   "source": [
    "### Normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yak6wTGtXgcA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_X  (1178, 132, 7) \n",
      "data_y (1178, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c82117316f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Splitting the data into the train, validation and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_val_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MTL_A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_regressive\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_F\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/Foresight/deeplearning.py\u001b[0m in \u001b[0;36mtraining_wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;31m# Train and validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/Foresight/deeplearning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# Perform backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;31m# Adding the predictions for this batch to prediction list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for forecast_length in [5, 22, 66, 132]:\n",
    "\n",
    "    # Data scaling\n",
    "    scaler_data_X = MinMaxScaler()\n",
    "    scaler_data_y = MinMaxScaler()\n",
    "\n",
    "    df_target = df_full[target_col]\n",
    "\n",
    "    data_X = scaler_data_X.fit_transform(df_full)[:-forecast_length, :]\n",
    "    # Need to have an independent scaler for inverse_transforming later\n",
    "    data_y = scaler_data_y.fit_transform(df_target)\n",
    "\n",
    "    # Offset target one forecast length\n",
    "    data_y = data_y[forecast_length:, :]\n",
    "\n",
    "    # Reducing the dataset to containing a threshold amount of variance\n",
    "    if use_PCA:\n",
    "        n_dim = dimension_selector(data_X, thresh=0.95, verbose=False)\n",
    "        data_X = dimension_reduce(data_X, n_dim, verbose=False)\n",
    "\n",
    "    # The input size of each time series window\n",
    "    series_length = 132\n",
    "\n",
    "    data_X, data_y = slice_series(data_X, data_y, series_length)\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 1e-2\n",
    "    momentum = 0.5\n",
    "    weight_decay = 0\n",
    "\n",
    "    # Batch Parameters\n",
    "    batch_size = 32\n",
    "\n",
    "    # Training Parameters\n",
    "    n_epochs = 20000\n",
    "    patience = 100\n",
    "    disp_freq= 50\n",
    "    fig_disp_freq= 50\n",
    "\n",
    "    # Model Parameters\n",
    "    num_features = data_X.shape[2]\n",
    "    hidden_dim = 8\n",
    "    dense_hidden = 8\n",
    "    dense_hidden_2 = 8\n",
    "    num_layers = 2\n",
    "    output_dim = data_y.shape[1]\n",
    "    dropout = 0.0\n",
    "\n",
    "    model = LSTM_deeper(num_features=num_features, \n",
    "                 hidden_dim=hidden_dim,\n",
    "                 dense_hidden=dense_hidden,\n",
    "                 dense_hidden_2=dense_hidden_2,\n",
    "                 series_length = series_length,\n",
    "                 batch_size=batch_size,\n",
    "                 output_dim=output_dim, \n",
    "                 num_layers=num_layers, \n",
    "                 device=device,\n",
    "                 dropout=dropout)\n",
    "\n",
    "    if parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(\"Parallel Workflow\\n\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print('data_X ', data_X.shape, '\\ndata_y', data_y.shape)\n",
    "\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    \"\"\"Do I need to declare a data explicitly in the dataset\"\"\"\n",
    "    learning = DeepLearning(model=model, \n",
    "                            data_X=data_X, \n",
    "                            data_y=data_y, \n",
    "                            n_epochs=n_epochs,\n",
    "                            optimiser=optimiser,\n",
    "                            batch_size=batch_size,\n",
    "                            debug=False, \n",
    "                            disp_freq=disp_freq,\n",
    "                            fig_disp_freq=0, \n",
    "                            device=device,\n",
    "                            patience=patience, \n",
    "                            scaler_data_X=scaler_data_X,\n",
    "                            scaler_data_y=scaler_data_y)\n",
    "\n",
    "\n",
    "    # Splitting the data into the train, validation and test sets\n",
    "    learning.train_val_test()\n",
    "    learning.training_wrapper()\n",
    "    learning.evaluate(learning.best_model, learning.test_loader)\n",
    "    model_name = \"MTL_A\" + str(auto_regressive) + \"_F\" + str(forecast_length)\n",
    "    print(model_name)\n",
    "\n",
    "    # Observed\n",
    "    train_true = learning.scaler_data_y.inverse_transform(learning.y_train.numpy())\n",
    "    val_true  = learning.scaler_data_y.inverse_transform(learning.y_val.numpy())\n",
    "    test_true  = learning.scaler_data_y.inverse_transform(learning.y_test.numpy())\n",
    "\n",
    "    # Predicted\n",
    "    train_pred = learning.scaler_data_y.inverse_transform(np.array(learning.train_predictions))\n",
    "    val_pred  = learning.scaler_data_y.inverse_transform(np.array(learning.val_predictions))\n",
    "    test_pred = learning.scaler_data_y.inverse_transform(np.array(learning.test_predictions))\n",
    "\n",
    "\n",
    "#     # Rescaling Example\n",
    "#     fig, ax = plt.subplots(2, 3, figsize=(16, 12))\n",
    "\n",
    "#     ax[0, 0].set_title(\"Training Predictions\")\n",
    "#     ax[0, 0].plot(train_true, label=\"Observed\")\n",
    "#     ax[0, 0].plot(train_pred, '--', label=\"Predicted\")\n",
    "#     ax[0, 0].grid()\n",
    "#     ax[0, 0].legend()\n",
    "\n",
    "#     ax[0, 1].grid()\n",
    "#     ax[0, 1].set_title(\"Validation Predictions\")\n",
    "#     ax[0, 1].plot(val_true, label=\"Observed\")\n",
    "#     ax[0, 1].plot(val_pred, '--', label=\"Predictions\")\n",
    "#     ax[0, 1].legend()\n",
    "\n",
    "#     ax[1, 0].grid()\n",
    "#     ax[1, 0].set_title(\"Test Predictions\")\n",
    "#     ax[1, 0].plot(test_true, label=\"Observed\")\n",
    "#     ax[1, 0].plot(test_pred, '--', label=\"Predictions\")\n",
    "#     ax[1, 0].legend()\n",
    "\n",
    "#     ax[1, 1].grid()\n",
    "#     ax[1, 1].set_title(\"Loss Plots\")\n",
    "#     ax[1, 1].plot(learning.logs['Training Loss'], label=\"Training Loss\")\n",
    "#     ax[1, 1].plot(learning.logs['Validation Loss'], label=\"Validation Loss\")\n",
    "#     ax[1, 1].legend()\n",
    "\n",
    "#     ax[0, 2].grid()\n",
    "#     ax[0, 2].set_title(\"Train Predictions\")\n",
    "#     ax[0, 2].plot(train_true[:, 0], label=\"Observed\")\n",
    "#     ax[0, 2].plot(train_pred[:, 0], '--', label=\"Predictions\")\n",
    "#     ax[0, 2].legend()\n",
    "\n",
    "\n",
    "#     ax[1, 2].grid()\n",
    "#     ax[1, 2].set_title(\"Test Predictions\")\n",
    "#     ax[1, 2].plot(test_true[:, 0], label=\"Observed\")\n",
    "#     ax[1, 2].plot(test_pred[:, 0], '--', label=\"Predictions\")\n",
    "#     ax[1, 2].legend()\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "    mse, mae, mde = evaluate(test_pred[:, :1], test_true[:, :1], log_ret=False)\n",
    "    print(\"Copper Price Metrics: \", \n",
    "          mse, mae, mde)\n",
    "\n",
    "    mse, mae, mde = evaluate(test_pred[:, :], test_true[:, :], log_ret=False)\n",
    "    print(\"Complex Price Metrics: \", \n",
    "          mse, mae, mde)\n",
    "\n",
    "    test_naive =  test_true[:-forecast_length, :]\n",
    "    mse_naive, mae_naive, mde_naive = evaluate(test_naive, test_true[forecast_length:], log_ret=False)\n",
    "    print(\"Naive: \", mse_naive, mae_naive, mde_naive)\n",
    "\n",
    "#     # Rescaling Example\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "\n",
    "#     ax[0].grid()\n",
    "#     ax[0].set_title(\"Test Predictions\")\n",
    "#     ax[0].plot(test_true[:-forecast_length], label=\"Observed\")\n",
    "#     ax[0].plot(test_naive, '--', label=\"Predictions\")\n",
    "#     ax[0].legend()\n",
    "\n",
    "\n",
    "#     ax[1].grid()\n",
    "#     ax[1].set_title(\"Test Predictions\")\n",
    "#     ax[1].plot(test_true[:,0][:], label=\"Observed\")\n",
    "#     ax[1].plot(test_naive[:,0], '--', label=\"Predictions\")\n",
    "#     ax[1].legend()\n",
    "\n",
    "    path = \"Results/Plots/\"\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbd87c05b4a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_naive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mforecast_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmse_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmde_naive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mforecast_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_ret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Naive: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmde_naive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Rescaling Example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_true' is not defined"
     ]
    }
   ],
   "source": [
    "test_naive = test_true[:-forecast_length, :]\n",
    "mse_naive, mae_naive, mde_naive = evaluate(test_naive, test_true[:-forecast_length], log_ret=False)\n",
    "print(\"Naive: \", mse_naive, mae_naive, mde_naive)\n",
    "\n",
    "# Rescaling Example\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "\n",
    "ax[0].grid()\n",
    "ax[0].set_title(\"Test Predictions\")\n",
    "ax[0].plot(test_true[forecast_length:], label=\"Observed\")\n",
    "ax[0].plot(test_naive, '--', label=\"Predictions\")\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].grid()\n",
    "ax[1].set_title(\"Test Predictions\")\n",
    "ax[1].plot(test_true[:,0][:], label=\"Observed\")\n",
    "ax[1].plot(test_naive[:,0], '--', label=\"Predictions\")\n",
    "ax[1].legend()\n",
    "\n",
    "path = \"Results/Plots/\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_lg_ret.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
