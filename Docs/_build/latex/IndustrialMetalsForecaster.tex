%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{Industrial Metals Forecaster Documentation}
\date{Aug 23, 2019}
\release{1.0.0}
\author{Oliver Boom}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Contents}
\label{\detokenize{index:contents}}

\section{Introduction}
\label{\detokenize{intro:introduction}}\label{\detokenize{intro::doc}}

\subsection{What is Foresight?}
\label{\detokenize{intro:what-is-foresight}}
Foresight is a collection of tools built to forecast the future price movements of industrial metals, using Long Short Term Memory networks. It can take univariate or multivariate datasets and make predictions using single-input single-output (SISO), multi-input single-output (MISO) or multi-input multi-output (MIMO) frameworks.

It was built for the purpose of testing the hypothesis that improved predictive performance can be achieved by applying the multi-task learning paradigm to commodity price forecasting. As such many of the example notebooks are built for this purpose.

The tools can equally be applied to any user chosen datasets, provided the datasets are loaded in the format shown in the example csvs, or are inputed directly as shown in the “generic” notebooks.


\subsection{Installation}
\label{\detokenize{intro:installation}}
To install:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{ForesightPy}
\end{sphinxVerbatim}

Ensure all requirements in requirements.txt are installed.

Example notebooks and datasets are contained within the source repo. This can be downloaded using the following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{msc}\PYG{o}{\PYGZhy{}}\PYG{n}{acse}\PYG{o}{/}\PYG{n}{acse}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{o}{\PYGZhy{}}\PYG{n}{independent}\PYG{o}{\PYGZhy{}}\PYG{n}{research}\PYG{o}{\PYGZhy{}}\PYG{n}{project}\PYG{o}{\PYGZhy{}}\PYG{n}{OliverJBoom}\PYG{o}{.}\PYG{n}{git}
\end{sphinxVerbatim}

To import package

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{ForesightPy} \PYG{k}{import} \PYG{o}{*}
\end{sphinxVerbatim}


\subsection{Requirements}
\label{\detokenize{intro:requirements}}
There are package dependancies on the following files:
\begin{itemize}
\item {} 
numpy==1.16.2

\item {} 
pandas==0.24.2

\item {} 
pmdarima==1.2.1

\item {} 
matplotlib==3.0.3

\item {} 
scikit-learn==0.20.3

\item {} 
statsmodels==0.9.0

\item {} 
torch==1.1.0

\end{itemize}


\subsection{Examples and Usage}
\label{\detokenize{intro:examples-and-usage}}
All examples can be found within the Notebooks folder.

Generic regression examples for univariate and multi-variate problems are contained within the “generic” notebooks.

For examples relating to industrial metal price forecasting; univariate, multivariate and multi-task examples can be found in the “metals forecaster” notebooks.


\section{Preprocessing Module}
\label{\detokenize{preprocessing:preprocessing-module}}\label{\detokenize{preprocessing::doc}}
Here are contained the functions related to the preprocessing of time series prior to any model training.

\phantomsection\label{\detokenize{preprocessing:module-Foresight.preprocessing}}\index{Foresight.preprocessing (module)@\spxentry{Foresight.preprocessing}\spxextra{module}}\index{clean\_data() (in module Foresight.preprocessing)@\spxentry{clean\_data()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.clean_data}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{clean\_data}}}{\emph{df}, \emph{n\_std=20}}{}
Removes any outliers that are further than a chosen
number of standard deviations from the mean.

These values are most likely wrongly inputted data,
and so are forward filled.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_std}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of standard deviations from the mean

\end{itemize}

\item[{Returns}] \leavevmode
The cleaned time series

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{clean\_dict\_gen() (in module Foresight.preprocessing)@\spxentry{clean\_dict\_gen()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.clean_dict_gen}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{clean\_dict\_gen}}}{\emph{universe\_dict}}{}
Generates a dictionary of cleaned DataFrames
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
The cleaned dictionary of time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{column\_rename() (in module Foresight.preprocessing)@\spxentry{column\_rename()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.column_rename}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{column\_rename}}}{\emph{universe\_dict}}{}
Appends the name of the instrument to the columns.
To help keep track of the instruments in the full dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
The dictionary of time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{dimension\_reduce() (in module Foresight.preprocessing)@\spxentry{dimension\_reduce()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.dimension_reduce}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{dimension\_reduce}}}{\emph{data\_X}, \emph{n\_dim}}{}
Performing PCA to reduce the dimensionality of the data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The dataset to perform reduction on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of dimensions to reduce to

\end{itemize}

\item[{Returns}] \leavevmode
The reduced dataset

\item[{Return type}] \leavevmode
np.array

\end{description}\end{quote}

\end{fulllineitems}

\index{dimension\_selector() (in module Foresight.preprocessing)@\spxentry{dimension\_selector()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.dimension_selector}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{dimension\_selector}}}{\emph{data\_X}, \emph{thresh=0.98}}{}
Calculated the number of dimensions required to reach a threshold level
of variance.

Completes a PCA reduction to an increasing number of dimensions
and calculates the total variance achieved for each reduction. If the
reduction is above the threshold then that number of dimensions is returned
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The dataset to perform reduction on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{thresh}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The amount of variance that must be
contained the in reduced dataset

\end{itemize}

\item[{Returns}] \leavevmode
The column dimensionality required to
contain the threshold variance

\item[{Return type}] \leavevmode
int

\end{description}\end{quote}

\end{fulllineitems}

\index{feature\_spawn() (in module Foresight.preprocessing)@\spxentry{feature\_spawn()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.feature_spawn}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{feature\_spawn}}}{\emph{df}}{}
Takes a time series and spawns several new features that explicitly
detail information about the series.

The DataFrame spawned contains the following features
spawned for each column in the input DataFrame:
\begin{quote}
\begin{description}
\item[{Exponentially Weighted Moving Average of various Half Lives:}] \leavevmode
1 day,
1 week,
1 month,
1 quarter,
6 months,
1 year

\item[{Rolling vol of different window sizes:}] \leavevmode
1 week,
1 month,
1 quarter

\end{description}
\end{quote}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} The dataset of independent variables

\item[{Returns}] \leavevmode
The DataFrame containing spawned features

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_dataset() (in module Foresight.preprocessing)@\spxentry{generate\_dataset()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.generate_dataset}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{generate\_dataset}}}{\emph{universe\_dict}, \emph{price\_only=True}, \emph{lg\_only=False}}{}
Generates the full dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag in days between series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lg\_only}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to return a dataset of log returns only

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{price\_only}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to return a dataset of raw prices only

\end{itemize}

\item[{Returns}] \leavevmode
The time series

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_lg\_return() (in module Foresight.preprocessing)@\spxentry{generate\_lg\_return()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.generate_lg_return}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{generate\_lg\_return}}}{\emph{df\_full}, \emph{lag=1}}{}
Creates the log return series for each column in the DataFrame
and returns the full dataset with log returns.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df\_full}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} The time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag between the series (in days)

\end{itemize}

\item[{Returns}] \leavevmode
The DataFrame of time series with log returns

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{log\_returns() (in module Foresight.preprocessing)@\spxentry{log\_returns()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.log_returns}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{log\_returns}}}{\emph{series}, \emph{lag=1}}{}
Calculates the log returns between adjacent close prices.
A constant lag is used across the whole series.
E.g a lag of one means a day to day log return.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} Prices to calculate the log returns on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag between the series (in days)

\end{itemize}

\item[{Returns}] \leavevmode
The series of log returns

\item[{Return type}] \leavevmode
np.array

\end{description}\end{quote}

\end{fulllineitems}

\index{price\_rename() (in module Foresight.preprocessing)@\spxentry{price\_rename()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.price_rename}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{price\_rename}}}{\emph{universe\_dict}}{}
Renaming the column of the DataFrame values to price.
This is actually the market closing price of the time series.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
The dictionary of renamed time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{slice\_series() (in module Foresight.preprocessing)@\spxentry{slice\_series()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.slice_series}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{slice\_series}}}{\emph{data\_X}, \emph{data\_y}, \emph{series\_len}, \emph{dataset\_pct=1.0}}{}
Slices the train and target dataset time series.

Turns each time series into a series of time series, with each series
displaced by one step forward to the previous series. And for each
of these windows there is an accompanying target value

The effect of this is to create an array of time series (which is the depth
equal to the amount of instruments in the dataset) with each entry in this
array having a target series in the data\_y array

The resulting data\_X array shape:
{[}amount of rolling windows, length of each series, number of instruments{]}

The resulting data\_y array shape:
{[}amount of rolling windows, number of instruments{]}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The dataset of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The target dataset of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series\_len}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The length of each time series window

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_pct}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The percentage of the full dataset to include

\end{itemize}

\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{truncate\_window\_length() (in module Foresight.preprocessing)@\spxentry{truncate\_window\_length()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.truncate_window_length}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{truncate\_window\_length}}}{\emph{universe\_dict}}{}
Chopping the length of all of the DataFrames to ensure
that they are all between the same dates.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
the dictionary of truncated time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{universe\_select() (in module Foresight.preprocessing)@\spxentry{universe\_select()}\spxextra{in module Foresight.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{preprocessing:Foresight.preprocessing.universe_select}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.preprocessing.}}\sphinxbfcode{\sphinxupquote{universe\_select}}}{\emph{path}, \emph{commodity\_name}, \emph{custom\_list=None}}{}
Selects the financial time series relevant for the commodities selected.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} path to the folder containing csvs

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{commodity\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} the name of the metal/s being inspected

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{custom\_list}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}) \textendash{} the names of csvs to be included in the dataset

\end{itemize}

\item[{Returns}] \leavevmode
The time series relevant to the commodities

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}



\section{Deeplearning Module}
\label{\detokenize{deeplearning:deeplearning-module}}\label{\detokenize{deeplearning::doc}}
Here are contained the set of functions relating to the training,
validation and testing of the neural networks.

\phantomsection\label{\detokenize{deeplearning:module-Foresight.deeplearning}}\index{Foresight.deeplearning (module)@\spxentry{Foresight.deeplearning}\spxextra{module}}\index{DeepLearning (class in Foresight.deeplearning)@\spxentry{DeepLearning}\spxextra{class in Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{DeepLearning}}}{\emph{model}, \emph{data\_X}, \emph{data\_y}, \emph{optimiser}, \emph{batch\_size=128}, \emph{n\_epochs=100}, \emph{loss\_function=\textless{}sphinx.ext.autodoc.importer.\_MockObject object\textgreater{}}, \emph{device='cpu'}, \emph{seed=42}, \emph{debug=True}, \emph{disp\_freq=20}, \emph{fig\_disp\_freq=50}, \emph{early\_stop=True}, \emph{early\_verbose=False}, \emph{patience=50}, \emph{rel\_tol=0}, \emph{scaler\_data\_X=None}, \emph{scaler\_data\_y=None}}{}
Class to perform training and validation for a given model
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{nn.module}}) \textendash{} The neural network model

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The training dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} the target dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_epochs}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of epochs of training

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{optimiser}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.optim}}) \textendash{} The type of optimiser used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The batch size

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{loss\_function}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.modules.loss}}) \textendash{} The loss function used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The device to run on (Cpu or CUDA)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number that is set for the random seeds

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{debug}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to print some parameters for checking

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{disp\_freq}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The epoch frequency that training/validation
metrics will be printed on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{fig\_disp\_freq}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The frequency that training/validation prediction
figures will be made

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{early\_stop}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether early stopping is utilized

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{early\_verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to print out the early stopping counter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{patience}} (\sphinxstyleliteralemphasis{\sphinxupquote{stopping int}}) \textendash{} The amount of epochs without improvement before

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{rel\_tol}} \textendash{} The relative improvement percentage that must be
achieved float

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{scaler\_data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{sklearn.preprocessing.data.MinMaxScaler}}) \textendash{} The data X scaler object for inverse scaling

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{scaler\_data\_y}} (\sphinxstyleliteralemphasis{\sphinxupquote{sklearn.preprocessing.data.MinMaxScaler}}) \textendash{} The dataX y scaler object for inverse scaling

\end{itemize}

\end{description}\end{quote}
\index{create\_data\_loaders() (Foresight.deeplearning.DeepLearning method)@\spxentry{create\_data\_loaders()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.create_data_loaders}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_data\_loaders}}}{}{}
Forms iterators to pipeline in the data/labels

\end{fulllineitems}

\index{evaluate() (Foresight.deeplearning.DeepLearning method)@\spxentry{evaluate()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.evaluate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{evaluate}}}{\emph{model}, \emph{test\_loader}}{}
Evaluates the performance of the network on given data for a given
model.

A lot of overlap of code with validation. Only kept separate due to the
inspection of attributes being made easier when running simulations
if kept separate.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{nn.module}}) \textendash{} The model to evaluate

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{test\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.utils.data.dataloader.DataLoader}}) \textendash{} The iterator that feeds in the data of choice

\end{itemize}

\item[{Returns}] \leavevmode
The error metric for that dataset

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{live\_pred\_plot() (Foresight.deeplearning.DeepLearning method)@\spxentry{live\_pred\_plot()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.live_pred_plot}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{live\_pred\_plot}}}{}{}
Plots the training predictions, validation predictions and the
training/validation losses as they are predicted.

\end{fulllineitems}

\index{size\_check() (Foresight.deeplearning.DeepLearning method)@\spxentry{size\_check()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.size_check}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{size\_check}}}{}{}
Checks the size of the datasets

\end{fulllineitems}

\index{train() (Foresight.deeplearning.DeepLearning method)@\spxentry{train()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{train\_loader}}{}
Performs a single training epoch and returns the loss metric
for the training dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{train\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.utils.data.dataloader.DataLoader}}) \textendash{} The iterator that feeds in the training data

\item[{Returns}] \leavevmode
The error metric for that epoch

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_val\_test() (Foresight.deeplearning.DeepLearning method)@\spxentry{train\_val\_test()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.train_val_test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_val\_test}}}{}{}
Splits the DataFrames in to a training, validation
and test set and creates torch tensors from the underlying
numpy arrays

\end{fulllineitems}

\index{training\_wrapper() (Foresight.deeplearning.DeepLearning method)@\spxentry{training\_wrapper()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.training_wrapper}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_wrapper}}}{}{}
The wrapper that performs the training and validation

\end{fulllineitems}

\index{validate() (Foresight.deeplearning.DeepLearning method)@\spxentry{validate()}\spxextra{Foresight.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.DeepLearning.validate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{validate}}}{\emph{val\_loader}}{}
Evaluates the performance of the network on unseen validation data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{val\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.utils.data.dataloader.DataLoader}}) \textendash{} the iterator that feeds in the validation data

\item[{Returns}] \leavevmode
the error metric for that epoch

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{early\_stopping (class in Foresight.deeplearning)@\spxentry{early\_stopping}\spxextra{class in Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.early_stopping}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{early\_stopping}}}{\emph{patience}, \emph{rel\_tol}, \emph{verbose=True}}{}
Used to facilitate early stopping during the training
of neural networks.

When called if the validation accuracy has not relative improved below a
relative tolerance set by the user the a counter is incremented. If the
counter passes a set value then the stop attribute is set to true. This
should be used as a break condition in the training loop.

If rel\_tol is set to 0 then the metric just needs to improve from it’s
existing value
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{patience}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The amount of epochs without improvement before stopping

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{rel\_tol}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The relative improvement \% that must be achieved

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to print the count number

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{best}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The best score achieved so far

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{counter}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The amount of epochs without improvement so far

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{stop}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether stopping criteria is achieved

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{full\_save() (in module Foresight.deeplearning)@\spxentry{full\_save()}\spxextra{in module Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.full_save}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{full\_save}}}{\emph{model}, \emph{model\_name}, \emph{optimiser}, \emph{num\_epoch}, \emph{learning\_rate}, \emph{momentum}, \emph{weight\_decay}, \emph{use\_lg\_returns}, \emph{PCA\_used}, \emph{data\_X}, \emph{train\_loss}, \emph{val\_loss}, \emph{test\_loss}, \emph{train\_time}, \emph{hidden\_dim}, \emph{mse}, \emph{mae}, \emph{mde}, \emph{path='Models/CSVs/'}}{}
Saves the models run details and hyper-parameters to a csv file
:param model:               The model run
:type  model:               nn.module
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{strin}}) \textendash{} The name the model is saved under

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{optimiser}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.optim}}) \textendash{} The optimiser type used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_epoch}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of epochs run for

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The learning rate learning hyper-parameter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{momentum}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The momentum learning hyper-parameter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{weight\_decay}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The weight decay learning hyper-parameter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{use\_lg\_returns}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether log returns was used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{PCA\_used}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether PCA was used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The training dataset (used to save the shape)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{train\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The loss on the training dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{val\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The loss on the validation dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{test\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The loss on the test dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{train\_time}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The amount of time to train

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the hidden layers

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mse}} (\sphinxstyleliteralemphasis{\sphinxupquote{floot}}) \textendash{} The mean squared error metric

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mae}} (\sphinxstyleliteralemphasis{\sphinxupquote{floot}}) \textendash{} The mean absolute error metric

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mde}} (\sphinxstyleliteralemphasis{\sphinxupquote{floot}}) \textendash{} The mean direction error metric

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The directory path to save in

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{model\_load() (in module Foresight.deeplearning)@\spxentry{model\_load()}\spxextra{in module Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.model_load}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{model\_load}}}{\emph{model\_name}, \emph{device}, \emph{path='Results/Pths/'}}{}
Loading function for the models.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The model name to load

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The directory path to load the model from

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{model\_save() (in module Foresight.deeplearning)@\spxentry{model\_save()}\spxextra{in module Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.model_save}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{model\_save}}}{\emph{model}, \emph{name}, \emph{path='Results/Pths/'}}{}
Saving function for the model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn}}) \textendash{} The model to save

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The name to save the model under

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The directory path to save the model in

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{param\_strip() (in module Foresight.deeplearning)@\spxentry{param\_strip()}\spxextra{in module Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.param_strip}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{param\_strip}}}{\emph{param}}{}
Strips the key text info out of certain parameters.
Used to save the text info of which models/optimiser objects are used
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{param}} (\sphinxstyleliteralemphasis{\sphinxupquote{object}}) \textendash{} The parameter object to find the name of

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_seed() (in module Foresight.deeplearning)@\spxentry{set\_seed()}\spxextra{in module Foresight.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{deeplearning:Foresight.deeplearning.set_seed}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.deeplearning.}}\sphinxbfcode{\sphinxupquote{set\_seed}}}{\emph{seed}}{}
Sets the random seeds to ensure deterministic behaviour.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number that is set for the random seeds

\item[{Returns}] \leavevmode
Confirmation that seeds have been set

\item[{Return type}] \leavevmode
bool

\end{description}\end{quote}

\end{fulllineitems}



\section{Models Module}
\label{\detokenize{models:models-module}}\label{\detokenize{models::doc}}
Here are contained the LSTM models developed for price prediction.
\index{LSTM (class in Foresight.models)@\spxentry{LSTM}\spxextra{class in Foresight.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Foresight.models.}}\sphinxbfcode{\sphinxupquote{LSTM}}}{\emph{num\_features}, \emph{hidden\_dim}, \emph{dense\_hidden}, \emph{output\_dim}, \emph{batch\_size}, \emph{series\_length}, \emph{device}, \emph{dropout=0.1}, \emph{num\_layers=2}}{}
A Long Short Term Memory network model with an additional dense layer
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_features}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of features in the dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the LSTMs hidden layer/s

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dense\_hidden}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the dense layers

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{output\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the output layer

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of items in each batch

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{Int}}) \textendash{} The length of the time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The device to run on (Cpu or CUDA)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dropout}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The probability of dropout

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_layers}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of stacked LSTM layers

\end{itemize}

\end{description}\end{quote}
\index{forward() (Foresight.models.LSTM method)@\spxentry{forward()}\spxextra{Foresight.models.LSTM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{x}}{}
Forward pass through the neural network
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} The input into the network

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_hidden() (Foresight.models.LSTM method)@\spxentry{init\_hidden()}\spxextra{Foresight.models.LSTM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM.init_hidden}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_hidden}}}{\emph{batch\_size}}{}
Initialised the hidden state to be zeros. This clears the hidden
state between batches. If you are running a stateful LSTM then this
needs to be changed.

To change to a stateful LSTM requires not detaching the backprop and
storing the computational graph. This strongly increases runtime and
shouldn’t make a big difference. Hence a stateful LSTM was not used.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The batch size to be zeroed

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{LSTM\_shallow (class in Foresight.models)@\spxentry{LSTM\_shallow}\spxextra{class in Foresight.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM_shallow}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Foresight.models.}}\sphinxbfcode{\sphinxupquote{LSTM\_shallow}}}{\emph{num\_features}, \emph{hidden\_dim}, \emph{output\_dim}, \emph{batch\_size}, \emph{series\_length}, \emph{device}, \emph{dropout=0.1}, \emph{num\_layers=2}}{}
A Long Short Term Memory network model that passes staight
from the LSTM layer to predictions
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_features}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of features in the dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the LSTMs hidden layer/s

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{output\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the output layer

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of items in each batch

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{Int}}) \textendash{} The length of the time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The device to run on (Cpu or CUDA)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dropout}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The probability of dropout

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_layers}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of stacked LSTM layers

\end{itemize}

\end{description}\end{quote}
\index{forward() (Foresight.models.LSTM\_shallow method)@\spxentry{forward()}\spxextra{Foresight.models.LSTM\_shallow method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM_shallow.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{x}}{}
Forward pass through the neural network
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} The input into the network

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_hidden() (Foresight.models.LSTM\_shallow method)@\spxentry{init\_hidden()}\spxextra{Foresight.models.LSTM\_shallow method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM_shallow.init_hidden}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_hidden}}}{\emph{batch\_size}}{}
Initialised the hidden state to be zeros. This clears the hidden
state between batches. If you are running a stateful LSTM then this
needs to be changed.

To change to a stateful LSTM requires not detaching the backprop and
storing the computational graph. This strongly increases runtime and
shouldn’t make a big difference. Hence a stateful LSTM was not used.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The batch size to be zeroed

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{LSTM\_deeper (class in Foresight.models)@\spxentry{LSTM\_deeper}\spxextra{class in Foresight.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM_deeper}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Foresight.models.}}\sphinxbfcode{\sphinxupquote{LSTM\_deeper}}}{\emph{num\_features}, \emph{hidden\_dim}, \emph{dense\_hidden}, \emph{dense\_hidden\_2}, \emph{output\_dim}, \emph{batch\_size}, \emph{series\_length}, \emph{device}, \emph{dropout=0.1}, \emph{num\_layers=2}}{}
A Long Short Term Memory network model with two additional dense layers
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_features}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of features in the dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the LSTMs hidden layer/s

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dense\_hidden}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the first dense layer

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dense\_hidden\_2}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the second dense layer

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{output\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the output layer

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of items in each batch

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{Int}}) \textendash{} The length of the time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The device to run on (Cpu or CUDA)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dropout}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The probability of dropout

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_layers}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of stacked LSTM layers

\end{itemize}

\end{description}\end{quote}
\index{forward() (Foresight.models.LSTM\_deeper method)@\spxentry{forward()}\spxextra{Foresight.models.LSTM\_deeper method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM_deeper.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{x}}{}
Forward pass through the neural network
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} The input into the network

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_hidden() (Foresight.models.LSTM\_deeper method)@\spxentry{init\_hidden()}\spxextra{Foresight.models.LSTM\_deeper method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{models:Foresight.models.LSTM_deeper.init_hidden}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_hidden}}}{\emph{batch\_size}}{}
Initialised the hidden state to be zeros. This clears the hidden
state between batches. If you are running a stateful LSTM then this
needs to be changed.

To change to a stateful LSTM requires not detaching the backprop and
storing the computational graph. This strongly increases runtime and
shouldn’t make a big difference. Hence a stateful LSTM was not used.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The batch size to be zeroed

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Evaluation and Inspection Module}
\label{\detokenize{eval_inspect:evaluation-and-inspection-module}}\label{\detokenize{eval_inspect::doc}}
Here are contained the functions that related to the evaluation of the performance of the networks predictions, and also functions that relate to the inspection of data.

\phantomsection\label{\detokenize{eval_inspect:module-Foresight.eval_inspect}}\index{Foresight.eval\_inspect (module)@\spxentry{Foresight.eval\_inspect}\spxextra{module}}\index{check\_day\_frequency() (in module Foresight.eval\_inspect)@\spxentry{check\_day\_frequency()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.check_day_frequency}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{check\_day\_frequency}}}{\emph{df}, \emph{col\_name='ds'}}{}
Creates a bar chart showing the frequency of the days of the week.

Used to check that only business days are included in the dataset, and
that there is a roughly equal distribution of entries across the week.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame containing the time series to check

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{col\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The name of the column of interest

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_length() (in module Foresight.eval\_inspect)@\spxentry{check\_length()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.check_length}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{check\_length}}}{\emph{universe\_dict}}{}
Checks the name of all the DataFrames in the dictionary of time series.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\end{description}\end{quote}

\end{fulllineitems}

\index{df\_std() (in module Foresight.eval\_inspect)@\spxentry{df\_std()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.df_std}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{df\_std}}}{\emph{df}, \emph{col\_name}}{}
Calculates standard deviation of a DataFrames column.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{col\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The column of interest

\end{itemize}

\item[{Returns}] \leavevmode
The standard deviation of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{evaluate() (in module Foresight.eval\_inspect)@\spxentry{evaluate()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.evaluate}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{evaluate}}}{\emph{y\_true}, \emph{y\_pred}, \emph{log\_ret=False}}{}
Calculates the error metrics for between two arrays.
\begin{description}
\item[{The error metrics calculated are:}] \leavevmode
Means Squared Error
Mean Absolute Error
Mean Directional Accuracy

\end{description}

For a log returns series the definition of mean directional accuracy
changes. This is as for a log return series it is the signum values of the
series that details which direction the series has moved. This is as a log
return series is the first difference of the original series. For raw price
The signal needs to be differenced before the signum function is applied.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{log\_ret}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether the series compared are log returns

\end{itemize}

\item[{Return error\_metrics}] \leavevmode
The error metrics of the series

\item[{Return type}] \leavevmode
List

\end{description}\end{quote}

\end{fulllineitems}

\index{inverse\_log\_returns() (in module Foresight.eval\_inspect)@\spxentry{inverse\_log\_returns()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.inverse_log_returns}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{inverse\_log\_returns}}}{\emph{original\_prices}, \emph{log\_returns}, \emph{lag=5}, \emph{offset=0}}{}
Takes a DataFrame of predicted log returns and original
prices and returns an array of predicted absolute prices

The offset parameter moves the series forwards or backwards to
align the series with the DataFrame it might be appended to.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{original\_prices}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame of absolute prices

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{log\_returns}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame of log returns

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag in days between series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{offset}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Amount to offset the series forwards of backwards

\end{itemize}

\item[{Returns}] \leavevmode
The raw prices given by the log returns

\item[{Return type}] \leavevmode
pd.Series

\end{description}\end{quote}

\end{fulllineitems}

\index{mean\_absolute\_percentage\_error() (in module Foresight.eval\_inspect)@\spxentry{mean\_absolute\_percentage\_error()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.mean_absolute_percentage_error}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{mean\_absolute\_percentage\_error}}}{\emph{y\_true}, \emph{y\_pred}}{}
Calculates the mean absolute percentage error between two arrays.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\end{itemize}

\item[{Returns}] \leavevmode
The mean absolute percentage error of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{mean\_directional\_accuracy() (in module Foresight.eval\_inspect)@\spxentry{mean\_directional\_accuracy()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.mean_directional_accuracy}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{mean\_directional\_accuracy}}}{\emph{y\_true}, \emph{y\_pred}}{}
Calculated the mean directional accuracy error metric
between two series.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\end{itemize}

\item[{Returns}] \leavevmode
The mean directional accuracy of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{mean\_directional\_accuracy\_log\_ret() (in module Foresight.eval\_inspect)@\spxentry{mean\_directional\_accuracy\_log\_ret()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.mean_directional_accuracy_log_ret}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{mean\_directional\_accuracy\_log\_ret}}}{\emph{y\_true}, \emph{y\_pred}}{}
Calculates the mean directional accuracy error metric between
two series of log returns.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\end{itemize}

\item[{Returns}] \leavevmode
The mean directional accuracy of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{visualise\_df() (in module Foresight.eval\_inspect)@\spxentry{visualise\_df()}\spxextra{in module Foresight.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{eval_inspect:Foresight.eval_inspect.visualise_df}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Foresight.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{visualise\_df}}}{\emph{df}}{}
Visualises each time series in a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} The DataFrame of time series to visualise

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{f}
\item\relax\sphinxstyleindexentry{Foresight.deeplearning}\sphinxstyleindexpageref{deeplearning:\detokenize{module-Foresight.deeplearning}}
\item\relax\sphinxstyleindexentry{Foresight.eval\_inspect}\sphinxstyleindexpageref{eval_inspect:\detokenize{module-Foresight.eval_inspect}}
\item\relax\sphinxstyleindexentry{Foresight.preprocessing}\sphinxstyleindexpageref{preprocessing:\detokenize{module-Foresight.preprocessing}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}