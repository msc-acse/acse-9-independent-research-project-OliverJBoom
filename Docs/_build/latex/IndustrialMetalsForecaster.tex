%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{Industrial Metals Forecaster Documentation}
\date{Aug 09, 2019}
\release{1.0.0}
\author{Oliver Boom}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Preprocessing Module}
\label{\detokenize{index:module-Forecaster.preprocessing}}\label{\detokenize{index:preprocessing-module}}\index{Forecaster.preprocessing (module)@\spxentry{Forecaster.preprocessing}\spxextra{module}}\index{clean\_data() (in module Forecaster.preprocessing)@\spxentry{clean\_data()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.clean_data}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{clean\_data}}}{\emph{df}, \emph{n\_std=20}}{}
Removes any outliers that are further than a chosen
number of standard deviations from the mean.

These values are most likely wrongly inputted data,
and so are forward filled.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_std}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of standard deviations from the mean

\end{itemize}

\item[{Returns}] \leavevmode
The cleaned time series

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{clean\_dict\_gen() (in module Forecaster.preprocessing)@\spxentry{clean\_dict\_gen()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.clean_dict_gen}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{clean\_dict\_gen}}}{\emph{universe\_dict}}{}
Generates a dictionary of cleaned DataFrames
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
The cleaned dictionary of time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{column\_rename() (in module Forecaster.preprocessing)@\spxentry{column\_rename()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.column_rename}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{column\_rename}}}{\emph{universe\_dict}}{}
Appends the name of the instrument to the columns.
To help keep track of the instruments in the full dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
The dictionary of time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{dimension\_reduce() (in module Forecaster.preprocessing)@\spxentry{dimension\_reduce()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.dimension_reduce}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{dimension\_reduce}}}{\emph{data\_X}, \emph{n\_dim}}{}
Performing PCA to reduce the dimensionality of the data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The dataset to perform reduction on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of dimensions to reduce to

\end{itemize}

\item[{Returns}] \leavevmode
The reduced dataset

\item[{Return type}] \leavevmode
np.array

\end{description}\end{quote}

\end{fulllineitems}

\index{dimension\_selector() (in module Forecaster.preprocessing)@\spxentry{dimension\_selector()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.dimension_selector}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{dimension\_selector}}}{\emph{data\_X}, \emph{thresh=0.98}}{}
Calculated the number of dimensions required to reach a threshold level
of variance.

Completes a PCA reduction to an increasing number of dimensions
and calculates the total variance achieved for each reduction. If the
reduction is above the threshold then that number of dimensions is returned
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The dataset to perform reduction on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{thresh}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The amount of variance that must be
contained the in reduced dataset

\end{itemize}

\item[{Returns}] \leavevmode
The column dimensionality required to
contain the threshold variance

\item[{Return type}] \leavevmode
int

\end{description}\end{quote}

\end{fulllineitems}

\index{feature\_spawn() (in module Forecaster.preprocessing)@\spxentry{feature\_spawn()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.feature_spawn}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{feature\_spawn}}}{\emph{df}}{}
Takes a time series and spawns several new features that explicitly
detail information about the series.

The DataFrame spawned contains the following features
spawned for each column in the input DataFrame:
\begin{quote}
\begin{description}
\item[{Exponentially Weighted Moving Average of various half lives:}] \leavevmode
Half Life:      1 day
Half Life:      1 week
Half Life:      1 month
Half Life:      1 quarter
Half Life:      6 months
Half Life:      1 year

\item[{Rolling vol of different window sizes:}] \leavevmode
Window Size:    1 week
Window Size:    1 month
Window Size:    1 quarter

\end{description}
\end{quote}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} The dataset of independent variables

\item[{Returns}] \leavevmode
The DataFrame containing spawned features

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_dataset() (in module Forecaster.preprocessing)@\spxentry{generate\_dataset()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.generate_dataset}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{generate\_dataset}}}{\emph{universe\_dict}, \emph{price\_only=True}, \emph{lg\_only=False}}{}
Generates the full dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag in days between series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lg\_only}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to return a dataset of log returns only

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{price\_only}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to return a dataset of raw prices only

\end{itemize}

\item[{Returns}] \leavevmode
The time series

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_lg\_return() (in module Forecaster.preprocessing)@\spxentry{generate\_lg\_return()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.generate_lg_return}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{generate\_lg\_return}}}{\emph{df\_full}, \emph{lag=1}}{}
Creates the log return series for each column in the DataFrame
and returns the full dataset with log returns.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df\_full}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} The time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag between the series (in days)

\end{itemize}

\item[{Returns}] \leavevmode
The DataFrame of time series with log returns

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{log\_returns() (in module Forecaster.preprocessing)@\spxentry{log\_returns()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.log_returns}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{log\_returns}}}{\emph{series}, \emph{lag=1}}{}
Calculates the log returns between adjacent close prices.
A constant lag is used across the whole series.
E.g a lag of one means a day to day log return.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} Prices to calculate the log returns on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag between the series (in days)

\end{itemize}

\item[{Returns}] \leavevmode
The series of log returns

\item[{Return type}] \leavevmode
np.array

\end{description}\end{quote}

\end{fulllineitems}

\index{price\_rename() (in module Forecaster.preprocessing)@\spxentry{price\_rename()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.price_rename}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{price\_rename}}}{\emph{universe\_dict}}{}
Renaming the column of the DataFrame values to price.
This is actually the market closing price of the time series.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
The dictionary of renamed time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{slice\_series() (in module Forecaster.preprocessing)@\spxentry{slice\_series()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.slice_series}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{slice\_series}}}{\emph{data\_X}, \emph{data\_y}, \emph{series\_len}, \emph{dataset\_pct=1.0}}{}
Slices the train and target dataset time series.

Turns each time series into a series of time series, with each series
displaced by one step forward to the previous series. And for each
of these windows there is an accompanying target value

The effect of this is to create an array of time series (which is the depth
equal to the amount of instruments in the dataset) with each entry in this
array having a target series in the data\_y array

The resulting data\_X array shape:
{[}amount of rolling windows, length of each series, number of instruments{]}

The resulting data\_y array shape:
{[}amount of rolling windows, number of instruments{]}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The dataset of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The target dataset of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series\_len}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The length of each time series window

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_pct}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The percentage of the full dataset to include

\end{itemize}

\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{truncate\_window\_length() (in module Forecaster.preprocessing)@\spxentry{truncate\_window\_length()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.truncate_window_length}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{truncate\_window\_length}}}{\emph{universe\_dict}}{}
Chopping the length of all of the DataFrames to ensure
that they are all between the same dates.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\item[{Returns}] \leavevmode
the dictionary of truncated time series

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{universe\_select() (in module Forecaster.preprocessing)@\spxentry{universe\_select()}\spxextra{in module Forecaster.preprocessing}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.preprocessing.universe_select}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.preprocessing.}}\sphinxbfcode{\sphinxupquote{universe\_select}}}{\emph{path}, \emph{commodity\_name}}{}
Selects the financial time series relevant for the commodities selected.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} path to the folder containing csvs

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{commodity\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} the name of the metal/s being inspected

\end{itemize}

\item[{Returns}] \leavevmode
The time series relevant to the commodities

\item[{Return type}] \leavevmode
dict

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Deeplearning Module}
\label{\detokenize{index:module-Forecaster.deeplearning}}\label{\detokenize{index:deeplearning-module}}\index{Forecaster.deeplearning (module)@\spxentry{Forecaster.deeplearning}\spxextra{module}}\index{DeepLearning (class in Forecaster.deeplearning)@\spxentry{DeepLearning}\spxextra{class in Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{DeepLearning}}}{\emph{model}, \emph{data\_X}, \emph{data\_y}, \emph{n\_epochs}, \emph{optimiser}, \emph{batch\_size}, \emph{loss\_function=MSELoss()}, \emph{device='cpu'}, \emph{seed=42}, \emph{debug=True}, \emph{disp\_freq=20}, \emph{fig\_disp\_freq=50}, \emph{early\_stop=True}, \emph{early\_verbose=False}, \emph{patience=50}, \emph{rel\_tol=0}, \emph{scaler\_data\_X=None}, \emph{scaler\_data\_y=None}}{}
Class to perform training and validation for a given model
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{nn.module}}) \textendash{} The neural network model

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The training dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} the target dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_epochs}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of epochs of training

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{optimiser}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.optim}}) \textendash{} The type of optimiser used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The batch size

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{loss\_function}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.modules.loss}}) \textendash{} The loss function used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The device to run on (Cpu or CUDA)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number that is set for the random seeds

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{debug}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to print some parameters for checking

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{disp\_freq}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The epoch frequency that training/validation
metrics will be printed on

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{fig\_disp\_freq}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The frequency that training/validation prediction
figures will be made

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{early\_stop}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether early stopping is utilized

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{early\_verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to print out the early stopping counter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{patience}} (\sphinxstyleliteralemphasis{\sphinxupquote{stopping int}}) \textendash{} The amount of epochs without improvement before

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{rel\_tol}} \textendash{} The relative improvement percentage that must be
achieved float

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{scaler\_data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{sklearn.preprocessing.data.MinMaxScaler}}) \textendash{} The data X scaler object for inverse scaling

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{scaler\_data\_y}} (\sphinxstyleliteralemphasis{\sphinxupquote{sklearn.preprocessing.data.MinMaxScaler}}) \textendash{} The dataX y scaler object for inverse scaling

\end{itemize}

\end{description}\end{quote}
\index{create\_data\_loaders() (Forecaster.deeplearning.DeepLearning method)@\spxentry{create\_data\_loaders()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.create_data_loaders}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_data\_loaders}}}{}{}
Forms iterators to pipeline in the data/labels

\end{fulllineitems}

\index{evaluate() (Forecaster.deeplearning.DeepLearning method)@\spxentry{evaluate()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.evaluate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{evaluate}}}{\emph{model}, \emph{test\_loader}}{}
Evaluates the performance of the network on given data for a given
model.

A lot of overlap of code with validation. Only kept separate due to the
inspection of attributes being made easier when running simulations
if kept separate.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{nn.module}}) \textendash{} The model to evaluate

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{test\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.utils.data.dataloader.DataLoader}}) \textendash{} The iterator that feeds in the data of choice

\end{itemize}

\item[{Returns}] \leavevmode
The error metric for that dataset

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{live\_pred\_plot() (Forecaster.deeplearning.DeepLearning method)@\spxentry{live\_pred\_plot()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.live_pred_plot}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{live\_pred\_plot}}}{}{}
Plots the training predictions, validation predictions and the
training/validation losses as they are predicted.

\end{fulllineitems}

\index{size\_check() (Forecaster.deeplearning.DeepLearning method)@\spxentry{size\_check()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.size_check}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{size\_check}}}{}{}
Checks the size of the datasets

\end{fulllineitems}

\index{train() (Forecaster.deeplearning.DeepLearning method)@\spxentry{train()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{train\_loader}}{}
Performs a single training epoch and returns the loss metric
for the training dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{train\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.utils.data.dataloader.DataLoader}}) \textendash{} The iterator that feeds in the training data

\item[{Returns}] \leavevmode
The error metric for that epoch

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{train\_val\_test() (Forecaster.deeplearning.DeepLearning method)@\spxentry{train\_val\_test()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.train_val_test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_val\_test}}}{}{}
Splits the DataFrames in to a training, validation
and test set and creates torch tensors from the underlying
numpy arrays

\end{fulllineitems}

\index{training\_wrapper() (Forecaster.deeplearning.DeepLearning method)@\spxentry{training\_wrapper()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.training_wrapper}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_wrapper}}}{}{}
The wrapper that performs the training and validation

\end{fulllineitems}

\index{validate() (Forecaster.deeplearning.DeepLearning method)@\spxentry{validate()}\spxextra{Forecaster.deeplearning.DeepLearning method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.DeepLearning.validate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{validate}}}{\emph{val\_loader}}{}
Evaluates the performance of the network on unseen validation data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{val\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.utils.data.dataloader.DataLoader}}) \textendash{} the iterator that feeds in the validation data

\item[{Returns}] \leavevmode
the error metric for that epoch

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{EarlyStopping (class in Forecaster.deeplearning)@\spxentry{EarlyStopping}\spxextra{class in Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.EarlyStopping}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{EarlyStopping}}}{\emph{patience}, \emph{rel\_tol}, \emph{verbose=False}}{}
Used to facilitate early stopping during the training
of neural networks.

When called if the validation accuracy has not relative improved below a
relative tolerance set by the user the a counter is incremented. If the
counter passes a set value then the stop attribute is set to true. This
should be used as a break condition in the training loop.

If rel\_tol is set to 0 then the metric just needs to improve from it’s
existing value
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{patience}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The amount of epochs without improvement before stopping

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{rel\_tol}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The relative improvement \% that must be achieved

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether to print the count number

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{best}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The best score achieved so far

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{counter}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The amount of epochs without improvement so far

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{stop}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether stopping criteria is achieved

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{full\_save() (in module Forecaster.deeplearning)@\spxentry{full\_save()}\spxextra{in module Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.full_save}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{full\_save}}}{\emph{model}, \emph{model\_name}, \emph{optimiser}, \emph{num\_epoch}, \emph{learning\_rate}, \emph{momentum}, \emph{weight\_decay}, \emph{use\_lg\_returns}, \emph{PCA\_used}, \emph{data\_X}, \emph{train\_loss}, \emph{val\_loss}, \emph{test\_loss}, \emph{train\_time}, \emph{hidden\_dim}, \emph{mse}, \emph{mae}, \emph{mde}, \emph{path='Models/CSVs/'}}{}
Saves the models run details and hyper-parameters to a csv file
:param model:               The model run
:type  model:               nn.module
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{strin}}) \textendash{} The name the model is saved under

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{optimiser}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.optim}}) \textendash{} The optimiser type used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_epoch}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of epochs run for

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The learning rate learning hyper-parameter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{momentum}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The momentum learning hyper-parameter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{weight\_decay}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The weight decay learning hyper-parameter

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{use\_lg\_returns}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether log returns was used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{PCA\_used}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether PCA was used

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The training dataset (used to save the shape)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{train\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The loss on the training dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{val\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The loss on the validation dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{test\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The loss on the test dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{train\_time}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The amount of time to train

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the hidden layers

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mse}} (\sphinxstyleliteralemphasis{\sphinxupquote{floot}}) \textendash{} The mean squared error metric

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mae}} (\sphinxstyleliteralemphasis{\sphinxupquote{floot}}) \textendash{} The mean absolute error metric

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mde}} (\sphinxstyleliteralemphasis{\sphinxupquote{floot}}) \textendash{} The mean direction error metric

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The directory path to save in

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{model\_load() (in module Forecaster.deeplearning)@\spxentry{model\_load()}\spxextra{in module Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.model_load}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{model\_load}}}{\emph{model\_name}, \emph{path='Results/Pths/'}}{}
Loading function for the models.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The model name to load

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The directory path to load the model from

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{model\_save() (in module Forecaster.deeplearning)@\spxentry{model\_save()}\spxextra{in module Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.model_save}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{model\_save}}}{\emph{model}, \emph{name}, \emph{path='Results/Pths/'}}{}
Saving function for the model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn}}) \textendash{} The model to save

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The name to save the model under

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The directory path to save the model in

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{param\_strip() (in module Forecaster.deeplearning)@\spxentry{param\_strip()}\spxextra{in module Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.param_strip}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{param\_strip}}}{\emph{param}}{}
Strips the key text info out of certain parameters.
Used to save the text info of which models/optimiser objects are used
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{param}} (\sphinxstyleliteralemphasis{\sphinxupquote{object}}) \textendash{} The parameter object to find the name of

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_seed() (in module Forecaster.deeplearning)@\spxentry{set\_seed()}\spxextra{in module Forecaster.deeplearning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.deeplearning.set_seed}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.deeplearning.}}\sphinxbfcode{\sphinxupquote{set\_seed}}}{\emph{seed}}{}
Sets the random seeds to ensure deterministic behaviour.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number that is set for the random seeds

\item[{Returns}] \leavevmode
Confirmation that seeds have been set

\item[{Return type}] \leavevmode
bool

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Evaluation and Inspection Module}
\label{\detokenize{index:module-Forecaster.eval_inspect}}\label{\detokenize{index:evaluation-and-inspection-module}}\index{Forecaster.eval\_inspect (module)@\spxentry{Forecaster.eval\_inspect}\spxextra{module}}\index{check\_day\_frequency() (in module Forecaster.eval\_inspect)@\spxentry{check\_day\_frequency()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.check_day_frequency}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{check\_day\_frequency}}}{\emph{df}, \emph{col\_name='ds'}}{}
Creates a bar chart showing the frequency of the days of the week.

Used to check that only business days are included in the dataset, and
that there is a roughly equal distribution of entries across the week.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame containing the time series to check

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{col\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The name of the column of interest

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_length() (in module Forecaster.eval\_inspect)@\spxentry{check\_length()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.check_length}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{check\_length}}}{\emph{universe\_dict}}{}
Checks the name of all the DataFrames in the dictionary of time series.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{universe\_dict}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} The dictionary of time series

\end{description}\end{quote}

\end{fulllineitems}

\index{df\_std() (in module Forecaster.eval\_inspect)@\spxentry{df\_std()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.df_std}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{df\_std}}}{\emph{df}, \emph{col\_name}}{}
Calculates standard deviation of a DataFrames column.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame of time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{col\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The column of interest

\end{itemize}

\item[{Returns}] \leavevmode
The standard deviation of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{evaluate() (in module Forecaster.eval\_inspect)@\spxentry{evaluate()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.evaluate}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{evaluate}}}{\emph{y\_true}, \emph{y\_pred}, \emph{log\_ret=False}}{}
Calculates the error metrics for between two arrays.
\begin{description}
\item[{The error metrics calculated are:}] \leavevmode
Means Squared Error
Mean Absolute Error
Mean Directional Accuracy

\end{description}

For a log returns series the definition of mean directional accuracy
changes. This is as for a log return series it is the signum values of the
series that details which direction the series has moved. This is as a log
return series is the first difference of the original series. For raw price
The signal needs to be differenced before the signum function is applied.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{log\_ret}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether the series compared are log returns

\end{itemize}

\item[{Return error\_metrics}] \leavevmode
The error metrics of the series

\item[{Return type}] \leavevmode
List

\end{description}\end{quote}

\end{fulllineitems}

\index{inverse\_log\_returns() (in module Forecaster.eval\_inspect)@\spxentry{inverse\_log\_returns()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.inverse_log_returns}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{inverse\_log\_returns}}}{\emph{original\_prices}, \emph{log\_returns}, \emph{lag=5}, \emph{offset=0}}{}
Takes a DataFrame of predicted log returns and original
prices and returns an array of predicted absolute prices

The offset parameter moves the series forwards or backwards to
align the series with the DataFrame it might be appended to.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{original\_prices}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame of absolute prices

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{log\_returns}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} A DataFrame of log returns

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The lag in days between series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{offset}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Amount to offset the series forwards of backwards

\end{itemize}

\item[{Returns}] \leavevmode
The raw prices given by the log returns

\item[{Return type}] \leavevmode
pd.Series

\end{description}\end{quote}

\end{fulllineitems}

\index{mean\_absolute\_percentage\_error() (in module Forecaster.eval\_inspect)@\spxentry{mean\_absolute\_percentage\_error()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.mean_absolute_percentage_error}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{mean\_absolute\_percentage\_error}}}{\emph{y\_true}, \emph{y\_pred}}{}
Calculates the mean absolute percentage error between two arrays.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\end{itemize}

\item[{Returns}] \leavevmode
The mean absolute percentage error of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{mean\_directional\_accuracy() (in module Forecaster.eval\_inspect)@\spxentry{mean\_directional\_accuracy()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.mean_directional_accuracy}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{mean\_directional\_accuracy}}}{\emph{y\_true}, \emph{y\_pred}}{}
Calculated the mean directional accuracy error metric
between two series.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\end{itemize}

\item[{Returns}] \leavevmode
The mean directional accuracy of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{mean\_directional\_accuracy\_log\_ret() (in module Forecaster.eval\_inspect)@\spxentry{mean\_directional\_accuracy\_log\_ret()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.mean_directional_accuracy_log_ret}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{mean\_directional\_accuracy\_log\_ret}}}{\emph{y\_true}, \emph{y\_pred}}{}
Calculates the mean directional accuracy error metric between
two series of log returns.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The observed values

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} The predicted values

\end{itemize}

\item[{Returns}] \leavevmode
The mean directional accuracy of the series

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{visualise\_df() (in module Forecaster.eval\_inspect)@\spxentry{visualise\_df()}\spxextra{in module Forecaster.eval\_inspect}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.eval_inspect.visualise_df}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{Forecaster.eval\_inspect.}}\sphinxbfcode{\sphinxupquote{visualise\_df}}}{\emph{df}}{}
Visualises each time series in a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{df}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} The DataFrame of time series to visualise

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Models Module}
\label{\detokenize{index:module-Forecaster.models}}\label{\detokenize{index:models-module}}\index{Forecaster.models (module)@\spxentry{Forecaster.models}\spxextra{module}}\index{LSTM (class in Forecaster.models)@\spxentry{LSTM}\spxextra{class in Forecaster.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.models.LSTM}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{Forecaster.models.}}\sphinxbfcode{\sphinxupquote{LSTM}}}{\emph{num\_features}, \emph{hidden\_dim}, \emph{dense\_hidden}, \emph{output\_dim}, \emph{batch\_size}, \emph{series\_length}, \emph{device}, \emph{dropout=0.1}, \emph{num\_layers=2}}{}
A Long Short Term Memory network model with an additional dense layer
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_features}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of features in the dataset

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the LSTMs hidden layer/s

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dense\_hidden}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the dense layers

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{output\_dim}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of neurons in the output layer

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of items in each batch

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{series\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{Int}}) \textendash{} The length of the time series

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The device to run on (Cpu or CUDA)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dropout}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} The probability of dropout

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_layers}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} The number of stacked LSTM layers

\end{itemize}

\end{description}\end{quote}
\index{forward() (Forecaster.models.LSTM method)@\spxentry{forward()}\spxextra{Forecaster.models.LSTM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.models.LSTM.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{x}}{}
Forward pass through the neural network
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} The input into the network

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_hidden() (Forecaster.models.LSTM method)@\spxentry{init\_hidden()}\spxextra{Forecaster.models.LSTM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:Forecaster.models.LSTM.init_hidden}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_hidden}}}{\emph{batch\_size}}{}
Initialised the hidden state to be zeros. This clears the hidden
state between batches. If you are running a stateful LSTM then this
needs to be changed.

To change to a stateful LSTM requires not detaching the backprop and
storing the computational graph. This strongly increases runtime and
shouldn’t make a big difference. Hence a stateful LSTM was not used.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The batch size to be zeroed

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{f}
\item\relax\sphinxstyleindexentry{Forecaster.deeplearning}\sphinxstyleindexpageref{index:\detokenize{module-Forecaster.deeplearning}}
\item\relax\sphinxstyleindexentry{Forecaster.eval\_inspect}\sphinxstyleindexpageref{index:\detokenize{module-Forecaster.eval_inspect}}
\item\relax\sphinxstyleindexentry{Forecaster.models}\sphinxstyleindexpageref{index:\detokenize{module-Forecaster.models}}
\item\relax\sphinxstyleindexentry{Forecaster.preprocessing}\sphinxstyleindexpageref{index:\detokenize{module-Forecaster.preprocessing}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}