{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67j8odKJJQL_"
   },
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd88JeQfrEzc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeplearning import *\n",
    "from livelossplot import PlotLosses\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from preprocessing import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import *\n",
    "import warnings\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def set_seed(seed, device='cpu'):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insuring that training is done on GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/\"\n",
    "universe_dict = universe_select(path, \"Cu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k8FIHASpHfv"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ki2dJCGNkOVE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included Instrument:\n",
      "cu_shfe\n",
      "cu_lme\n",
      "cu_comex_p\n",
      "cu_comex_s\n",
      "peso\n",
      "sol\n",
      "bdi\n",
      "ted\n",
      "vix\n",
      "skew\n",
      "gsci\n"
     ]
    }
   ],
   "source": [
    "# Renaming the columns to price\n",
    "universe_dict = price_rename(universe_dict)\n",
    "# Cleaning the dataset of any erroneous datapoints\n",
    "universe_dict = clean_dict_gen(universe_dict)\n",
    "# Making sure that all the points in the window have consistent lenght\n",
    "universe_dict = truncate_window_length(universe_dict)\n",
    "\n",
    "# Lg Returns Only\n",
    "# df_full = generate_dataset(universe_dict, lg_returns_only=True, price_only=False)\n",
    "# Price Only\n",
    "# df_full = generate_dataset(universe_dict, lg_returns_only=False, price_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both Price and Log Returns\n",
    "df_full = generate_dataset(universe_dict, lg_returns_only=False, price_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Price Target\n",
    "df_target = df_full[[\"price_cu_lme\"]]\n",
    "df_target = df_target.shift(-5)\n",
    "df_full = df_full[df_full.columns.drop(list(df_full.filter(regex='price')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target column represents the log returns at one forecast length out in the future for the instrument of interest (aluminium or copper prices on the London Metals Exchange). \n",
    "\n",
    "To normalise the independent variables, the 1 day log returns between closing prices have been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise the plots\n",
    "# visualise_universe(universe_dict)\n",
    "df = df_full[[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_cu_shfe</th>\n",
       "      <th>price_cu_lme</th>\n",
       "      <th>price_cu_comex_p</th>\n",
       "      <th>price_cu_comex_s</th>\n",
       "      <th>price_peso</th>\n",
       "      <th>price_sol</th>\n",
       "      <th>price_bdi</th>\n",
       "      <th>price_ted</th>\n",
       "      <th>price_vix</th>\n",
       "      <th>price_skew</th>\n",
       "      <th>...</th>\n",
       "      <th>cu_comex_p</th>\n",
       "      <th>cu_comex_s</th>\n",
       "      <th>peso</th>\n",
       "      <th>sol</th>\n",
       "      <th>bdi</th>\n",
       "      <th>ted</th>\n",
       "      <th>vix</th>\n",
       "      <th>skew</th>\n",
       "      <th>gsci</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-09-13</th>\n",
       "      <td>69540.0</td>\n",
       "      <td>7484.5</td>\n",
       "      <td>3.3925</td>\n",
       "      <td>15748.0</td>\n",
       "      <td>537.35</td>\n",
       "      <td>3.2530</td>\n",
       "      <td>4129.0</td>\n",
       "      <td>4.901</td>\n",
       "      <td>11.18</td>\n",
       "      <td>120.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.055554</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>-0.004276</td>\n",
       "      <td>-0.064091</td>\n",
       "      <td>-0.048458</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>-0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-14</th>\n",
       "      <td>71350.0</td>\n",
       "      <td>7439.0</td>\n",
       "      <td>3.3820</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>537.73</td>\n",
       "      <td>3.2475</td>\n",
       "      <td>4207.0</td>\n",
       "      <td>4.926</td>\n",
       "      <td>11.55</td>\n",
       "      <td>119.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-0.104881</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>-0.001692</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.032559</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>0.021675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-15</th>\n",
       "      <td>69400.0</td>\n",
       "      <td>7281.0</td>\n",
       "      <td>3.3165</td>\n",
       "      <td>14775.0</td>\n",
       "      <td>537.10</td>\n",
       "      <td>3.2480</td>\n",
       "      <td>4258.0</td>\n",
       "      <td>4.939</td>\n",
       "      <td>11.76</td>\n",
       "      <td>135.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019557</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>0.124060</td>\n",
       "      <td>-0.004952</td>\n",
       "      <td>0.043012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-18</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>7459.0</td>\n",
       "      <td>3.4180</td>\n",
       "      <td>15263.0</td>\n",
       "      <td>537.10</td>\n",
       "      <td>3.2450</td>\n",
       "      <td>4279.0</td>\n",
       "      <td>4.949</td>\n",
       "      <td>11.78</td>\n",
       "      <td>127.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030146</td>\n",
       "      <td>0.032495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>-0.060186</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.013450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-19</th>\n",
       "      <td>71470.0</td>\n",
       "      <td>7516.0</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>17523.0</td>\n",
       "      <td>537.10</td>\n",
       "      <td>3.2450</td>\n",
       "      <td>4275.0</td>\n",
       "      <td>4.948</td>\n",
       "      <td>11.98</td>\n",
       "      <td>125.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012512</td>\n",
       "      <td>0.138083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>-0.018385</td>\n",
       "      <td>-0.019642</td>\n",
       "      <td>0.025225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            price_cu_shfe  price_cu_lme  price_cu_comex_p  price_cu_comex_s  \\\n",
       "date                                                                          \n",
       "2006-09-13        69540.0        7484.5            3.3925           15748.0   \n",
       "2006-09-14        71350.0        7439.0            3.3820           14180.0   \n",
       "2006-09-15        69400.0        7281.0            3.3165           14775.0   \n",
       "2006-09-18        69830.0        7459.0            3.4180           15263.0   \n",
       "2006-09-19        71470.0        7516.0            3.3755           17523.0   \n",
       "\n",
       "            price_peso  price_sol  price_bdi  price_ted  price_vix  \\\n",
       "date                                                                 \n",
       "2006-09-13      537.35     3.2530     4129.0      4.901      11.18   \n",
       "2006-09-14      537.73     3.2475     4207.0      4.926      11.55   \n",
       "2006-09-15      537.10     3.2480     4258.0      4.939      11.76   \n",
       "2006-09-18      537.10     3.2450     4279.0      4.949      11.78   \n",
       "2006-09-19      537.10     3.2450     4275.0      4.948      11.98   \n",
       "\n",
       "            price_skew  ...  cu_comex_p  cu_comex_s      peso       sol  \\\n",
       "date                    ...                                               \n",
       "2006-09-13      120.44  ...    0.000590    0.055554 -0.000558  0.000307   \n",
       "2006-09-14      119.47  ...   -0.003100   -0.104881  0.000707 -0.001692   \n",
       "2006-09-15      135.25  ...   -0.019557    0.041104 -0.001172  0.000154   \n",
       "2006-09-18      127.35  ...    0.030146    0.032495  0.000000 -0.000924   \n",
       "2006-09-19      125.03  ...   -0.012512    0.138083  0.000000  0.000000   \n",
       "\n",
       "                 bdi       ted       vix      skew      gsci    target  \n",
       "date                                                                    \n",
       "2006-09-13  0.029244 -0.004276 -0.064091 -0.048458  0.002624 -0.001404  \n",
       "2006-09-14  0.018715  0.005088  0.032559 -0.008086 -0.010694  0.021675  \n",
       "2006-09-15  0.012050  0.002636  0.018019  0.124060 -0.004952  0.043012  \n",
       "2006-09-18  0.004920  0.002023  0.001699 -0.060186  0.007824  0.013450  \n",
       "2006-09-19 -0.000935 -0.000202  0.016835 -0.018385 -0.019642  0.025225  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head(5)\n",
    "# df_full.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise data\n",
    "\n",
    "Split into validation data and test data\n",
    "\n",
    "Use validation to tune hyperparameters\n",
    "\n",
    "Perform predictions on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_full.loc[:, df_full.columns != 'target']\n",
    "# df_y = df_full.loc[:, df_full.columns == 'target']\n",
    "df_y = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input size of each time series window\n",
    "series_length = 100\n",
    "\n",
    "# Creating a data structure which is slices of input size\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(series_length, len(df_X)):\n",
    "    X_train.append(df_X.values[i-series_length:i, :])\n",
    "    y_train.append(df_y.values[i, 0])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "momentum = 0.5\n",
    "weight_decay = 0\n",
    "\n",
    "# Batch Parameters\n",
    "batch_size = 128\n",
    "test_batch_size = 1000\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "# Model Parameters\n",
    "num_features = len(df_X.columns)\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for LSTM deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearning():\n",
    "    \"\"\"Class to perform training and validation for a given model\"\"\"\n",
    "    def __init__(self, model, df_X, df_y,\n",
    "                 n_epochs,\n",
    "                 optimiser, \n",
    "                 loss_function=torch.nn.MSELoss(size_average=False),\n",
    "                 window_size = 100,\n",
    "                 device=\"cpu\", \n",
    "                 seed=42,\n",
    "                 debug = True):\n",
    "        \n",
    "        # The neural network architecture\n",
    "        self.model = model\n",
    "        \n",
    "        # The optimiser for gradient descent\n",
    "        self.optimiser = optimiser\n",
    "        \n",
    "        # Dataframe of training values\n",
    "        self.df_X = df_X\n",
    "        \n",
    "        # Dataframe of target values\n",
    "        self.df_y = df_y\n",
    "        \n",
    "        # The number of epochs\n",
    "        self.n_epochs = n_epochs\n",
    "   \n",
    "        #self.optimiser = optimiser\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        # Whether to run on cpu or gpu\n",
    "        self.device = device\n",
    "        \n",
    "        # The random seed to set\n",
    "        self.seed = seed\n",
    "        \n",
    "        # To activate debug mode\n",
    "        self.debug = debug\n",
    "        \n",
    "        # For training logs\n",
    "        self.logs = None\n",
    "        \n",
    "        # The array of predicted lists for each batch\n",
    "        self.pred_list = None\n",
    "        \n",
    "        assert (type(self.df_X) == pd.DataFrame)\n",
    "        assert (type(self.df_y) == pd.DataFrame)\n",
    "        assert (len(self.df_X.index) == len(self.df_y.index))\n",
    "        assert (len(self.df_X.index) > 0)\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.inspect = None\n",
    "        \n",
    "    def train_val_test(self):\n",
    "        \"\"\"Splits the dataframes in to a training, validation\n",
    "        and test set and creates torch tensors from the underlying\n",
    "        numpy arrays\"\"\"\n",
    "        # Splitting the sets into train, test and validation\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.df_X, self.df_y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X_train, self.y_train, test_size=0.25, shuffle=False)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Train Length: \\t\\t%i\\nValidation Length: \\t%i\\nTest Length:\\t\\t%i\" \n",
    "                  % (len(X_train), len(X_val), len(X_test)))\n",
    "        \n",
    "        # Performs normalisation\n",
    "        # This changes the types of the X_train/val/test \n",
    "        # from dataframe to arrays\n",
    "        self.normalise()\n",
    "        \n",
    "        # Tensor of training data\n",
    "        self.X_train = torch.from_numpy(self.X_train).float()\n",
    "        self.y_train = torch.from_numpy(self.y_train.values).float()\n",
    "\n",
    "        # Tensor of training labels\n",
    "        self.X_val = torch.from_numpy(self.X_val).float()\n",
    "        self.y_val = torch.from_numpy(self.y_val.values).float()\n",
    "\n",
    "        #  Tensor of test data\n",
    "        self.X_test = torch.from_numpy(self.X_test).float()\n",
    "        self.y_test = torch.from_numpy(self.y_test.values).float()\n",
    "\n",
    "        # Size Check\n",
    "        if self.debug:\n",
    "            print(\"\\nInitial Size Check:\")\n",
    "            self.size_check()\n",
    "    \n",
    "    \n",
    "    def size_check(self):\n",
    "        \"\"\"Checks the size of the datasets\"\"\"\n",
    "        if self.debug:\n",
    "            print(\"\\nX Train Shape:\\t\\t\", self.X_train.size())\n",
    "            print(\"X Val Shape:\\t\\t\", self.X_val.size())\n",
    "            print(\"X Test Shape:\\t\\t\", self.X_test.size())\n",
    "\n",
    "            print(\"\\ny Train Shape:\\t\\t\", self.y_train.size())\n",
    "            print(\"y Val Shape:\\t\\t\", self.y_val.size())\n",
    "            print(\"y Test Shape:\\t\\t\", self.y_test.size())\n",
    "    \n",
    "    \n",
    "    def normalise(self):\n",
    "        \"\"\"Normalizes the data using MaxMinScaler, which\n",
    "            was chosen because it preserves the original \n",
    "            scale and doesn't reduce effect of outliers\"\"\"\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # Normalize the validation and test set by the same\n",
    "        # scale as the training data. Needed for training to\n",
    "        # be correctly scaled\n",
    "        \n",
    "        self.X_train = scaler.fit_transform(self.X_train)\n",
    "        self.X_val = scaler.transform(self.X_val)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "        \n",
    "        \"\"\"TODO implement output scaling for MTL because \n",
    "        otherwise the scale of the different outputs will\n",
    "        lead to dominance of some tasks\"\"\"\n",
    "        \n",
    "    \n",
    "    def create_data_loaders(self):\n",
    "        \"\"\"Forms iterators to pipeline in the data\"\"\"\n",
    "        \n",
    "        # Slices the datasets into their respective windows\n",
    "        # Beginning at 0 with a step of 1, then rearranging the \n",
    "        # columns to be consisten with notation.\n",
    "        self.X_train= self.X_train.unfold(0, self.window_size, 1).permute(0, 2, 1)\n",
    "        self.X_val = self.X_val.unfold(0, self.window_size, 1).permute(0, 2, 1)\n",
    "        self.X_test = self.X_test.unfold(0, self.window_size, 1).permute(0, 2, 1)\n",
    "        \n",
    "        # The target values also need to be lined up\n",
    "        # This reduces the dataset by the window size\n",
    "        # The [:,;,-1] it to take the value at the end\n",
    "        # of the window now the beginning\n",
    "        self.y_train = self.y_train.unfold(0, self.window_size, 1)[:, :, -1]\n",
    "        self.y_val = self.y_val.unfold(0, self.window_size, 1)[:, :, -1]\n",
    "        self.y_test = self.y_test.unfold(0, self.window_size, 1)[:, :, -1]\n",
    "        \n",
    "        print(\"\\nTime Series Quantity: %i Rolling Window Size: %i Feature Number: %i\"\n",
    "             %(self.X_train.shape[0], self.X_train.shape[1], self.X_train.shape[2]))\n",
    "        if self.debug:\n",
    "            print(\"\\nDataset Creation Size Check:\")\n",
    "            self.size_check()\n",
    "            \n",
    "        # Create tensor datasets\n",
    "        train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "        test_dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        \n",
    "        # Data loaders\n",
    "        train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_data_loader, val_data_loader, test_data_loader\n",
    "    \n",
    "    def train(self, train_data_loader):\n",
    "        \"\"\"Performs a single training cycle and returns the\n",
    "        mean squared error loss for the training dataset\"\"\"\n",
    "        # Sets the model to train mode\n",
    "        self.model.train()\n",
    "        \n",
    "        train_loss = 0.\n",
    "        \n",
    "        # List of each batch predictions\n",
    "        pred_list = []\n",
    "        \n",
    "        # The data loader creates batches of data to train\n",
    "        for i_batch, (X_train_batch, y_train_batch) in enumerate(train_data_loader):\n",
    "            \n",
    "            #print('Batch : ', i_batch)\n",
    "            \n",
    "            # Sending the data to GPU if available\n",
    "            X_train_batch = X_train_batch.to(self.device)\n",
    "            y_train_batch = y_train_batch.to(self.device)\n",
    "            \n",
    "            # Zeros the gradients\n",
    "            self.optimiser.zero_grad()\n",
    "\n",
    "            # Need to set seed here to make deterministic\n",
    "            set_seed(42)\n",
    "        \n",
    "            # Perform forward pass\n",
    "            y_pred = self.model(X_train_batch)\n",
    "            \n",
    "            # Calculate loss for the batch\n",
    "            loss = self.loss_function(y_pred, y_train_batch)   \n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()   \n",
    "\n",
    "            # Adding the predictions for this batch to prediction list\n",
    "            pred_list.append(y_pred)\n",
    "            \n",
    "            # Calculate the training loss\n",
    "            train_loss += (loss * X_train_batch.size()[0]).detach().cpu().numpy()\n",
    "\n",
    "            # Update Parameters\n",
    "            self.optimiser.step()               \n",
    "        \n",
    "        # Converting an array of batches of predictions to a list of predictions\n",
    "        self.pred_list = [single_pred for batch in pred_list for single_pred in batch.detach().numpy()]\n",
    "        \n",
    "        return train_loss/len(train_data_loader.dataset.tensors[0])\n",
    "    \n",
    "    \n",
    "#     def validate(self, val_data_loader):\n",
    "#         \"\"\"Evaluates the performance of the network\n",
    "#         on unseen validation data\"\"\"\n",
    "#         # Set the model to evaluate mode\n",
    "#         self.model.eval()\n",
    "        \n",
    "#         val_loss = 0.\n",
    "#         # The data loader creates batches of data to validate\n",
    "#         for X_val_batch, y_val_batch in val_data_loader:\n",
    "            \n",
    "#             # Ensures that the gradients are not updated\n",
    "#             with torch.no_grad():\n",
    "                \n",
    "#                 # Sending the data to GPU if available\n",
    "#                 X_val_batch = X_val_batch.to(self.device)\n",
    "#                 y_val_batch = y_val_batch.to(self.device)\n",
    "            \n",
    "#                 # Perform forward pass\n",
    "#                 y_pred = self.model(X_val_batch)\n",
    "\n",
    "#                 # Calculate loss for the batch\n",
    "#                 loss = self.loss_function(y_pred, y_val_batch) \n",
    "                \n",
    "#                 # Calculate the validation loss\n",
    "#                 val_loss += (loss * X_val_batch.size()[0]).detach().cpu().numpy()\n",
    "            \n",
    "#         return val_loss/len(val_data_loader.dataset.tensors[0])\n",
    "    \n",
    "    \n",
    "    def training_wrapper(self):\n",
    "        \n",
    "        # start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # set seed\n",
    "        set_seed(int(self.seed))\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader, test_loader = learning.create_data_loaders()\n",
    "        \n",
    "        # Shows live plot of losses\n",
    "        liveloss = PlotLosses()\n",
    "        \n",
    "        # Begin training\n",
    "        for epoch in range(self.n_epochs):\n",
    "            \n",
    "            logs = {}\n",
    "            \n",
    "            train_loss = self.train(train_loader) \n",
    "            #val_loss = self.validate(val_loader) \n",
    "            \n",
    "            print(\"Losses:\" , train_loss.item())\n",
    "            #print(\"Losses:\" ,train_loss.item(), val_loss.item())\n",
    "            \n",
    "            logs['' + 'log loss'] = train_loss.item()\n",
    "            #logs['val_' + 'log loss'] = val_loss.item()\n",
    "            logs['time'] = time.time() - start_time\n",
    "            \n",
    "            liveloss.update(logs)\n",
    "            liveloss.draw()\n",
    "            \n",
    "            print(\"Epoch %i MSE %.2f  Time %.1f\" % (epoch, train_loss.item(), (time.time() - start_time)))\n",
    "    \n",
    "    \"\"\"TODO Add in observed list and pred list\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"A Long Short Term Memory network\n",
    "    model\"\"\"\n",
    "        \n",
    "    def __init__(self, num_features, hidden_dim, output_dim,\n",
    "                 batch_size, series_length, \n",
    "                 dropout=0.1, num_layers=2, debug=True):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Number of features\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # The output dimensions\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Batch Size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Length of sequence\n",
    "        self.series_length = series_length\n",
    "        \n",
    "        # Dropout Probability\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.num_features, \n",
    "            hidden_size =self.hidden_dim,\n",
    "            num_layers =self.num_layers)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "        \"\"\"TODO Add fully connected layer and see if that improves\"\"\"\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialised the hidden state to be zeros\"\"\"\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the neural network\"\"\"\n",
    "        \n",
    "        \"\"\"TODO Directly switch these variables in \n",
    "        the permute of the dataset\"\"\"\n",
    "        \n",
    "        # Adjust to a variable batch size \n",
    "        batch_size = x.size()[0]\n",
    "        series_length = x.size()[1]\n",
    "        \n",
    "        assert (series_length == self.series_length)\n",
    "        \n",
    "        \"\"\"TODO Check output of contiguous and non \n",
    "        contigious memory\"\"\"\n",
    "        # just to be sure of the dimension\n",
    "        x = x.contiguous().view(series_length, batch_size, -1) \n",
    "\n",
    "        # Initialises the hidden states\n",
    "        h0, c0 = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Pass through through lstm layer\n",
    "        # Only the x is of interest\n",
    "        x, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Output is seq to seq but only want seq to val\n",
    "        # So only use the final value of the lstm outputted\n",
    "        # sequence\n",
    "        x = x[-1]  \n",
    "\n",
    "        return self.out(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2, debug=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        print(num_features, hidden_dim, batch_size)\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = self.num_features, \n",
    "            hidden_size =self.hidden_dim,\n",
    "            num_layers =self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This initialised the hidden state to be zeros\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the neural network\"\"\"\n",
    "        \n",
    "        # Adjust to a variable batch size \n",
    "        batch_size = x.size()[0]\n",
    "        seq_length = x.size()[1]\n",
    "        #         print(\"seq_length, batch_size\",\n",
    "        #              seq_length, batch_size)\n",
    "        \n",
    "        \"\"\"TODO Check output of contiguous and non \n",
    "        contigious memory\"\"\"\n",
    "        # just to be sure of the dimension\n",
    "        x = x.contiguous().view(seq_length, batch_size, -1) \n",
    "        \n",
    "        # Initialises the hidden states\n",
    "        h0, c0 = self.init_hidden()\n",
    "        # Passes through the lstm layer with hidden states\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        #lstm_out, self.hidden = self.lstm(x.view(len(x), self.batch_size, -1))\n",
    "        print(\"x\", out[-1].view(batch_size, -1).shape)\n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(out[-1].view(batch_size, -1))\n",
    "\n",
    "        return y_pred.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n",
      "torch.Size([50, 100, 11]) torch.Size([50, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-956-c6f22ece82e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Loss Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-650-968a8b655cdf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Passes through the lstm layer with hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# shape of lstm_out: [input_size, batch_size, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data loader version\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "num_epochs = 10\n",
    "# num_epochs = 500\n",
    "loss_log = np.zeros(num_epochs)\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model_test.parameters(), learning_rate)\n",
    "\n",
    "observed_list = []\n",
    "start_time = time.time()\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    \n",
    "    pred_list = []\n",
    "    \n",
    "    for batch_idx, (X_train, y_train) in enumerate(train_loader):\n",
    "        print(X_train.shape, y_train.shape)\n",
    "        if t == 0: observed_list.append(y_train)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Need to set seed here to make deterministic\n",
    "        set_seed(42)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model_test(X_train)\n",
    "\n",
    "        # Loss Function\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        loss_log[t] = loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "        pred_list.append(y_pred)\n",
    "\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"Time:\", (time.time() - start_time))\n",
    "#     if t % 100 == 0: print(\"Epoch \", t, \"MSE: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time Series Quantity: 1900 Rolling Window Size: 100 Feature Number: 11\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n",
      "x torch.Size([50, 64])\n",
      "out torch.Size([50])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1074-e7aac5e93dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Splitting the data into the train, validation and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_val_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1034-8ce43e982882>\u001b[0m in \u001b[0;36mtraining_wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;31m#val_loss = self.validate(val_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1034-8ce43e982882>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data_loader)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# Perform forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Calculate loss for the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1072-62d51ddb6d38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Pass through through lstm layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Only the x is of interest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Output is seq to seq but only want seq to val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_X = df_full.loc[:, df_full.columns != 'target']\n",
    "# df_y = df_full.loc[:, df_full.columns == 'target']\n",
    "# df_y = df_target\n",
    "\n",
    "\n",
    "# optimiser = torch.optim.SGD(model_test.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "optimiser = torch.optim.Adam(model_test.parameters(), learning_rate)\n",
    "\n",
    "\"\"\"Do I need to declare a data explicitly in the dataset\"\"\"\n",
    "learning = DeepLearning(model=model_test, \n",
    "                        df_X=df_X, \n",
    "                        df_y=df_y, \n",
    "                        n_epochs=3,\n",
    "                        optimiser=optimiser,\n",
    "                        debug=False)\n",
    "\n",
    "# Splitting the data into the train, validation and test sets\n",
    "learning.train_val_test()\n",
    "learning.training_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non class runthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = learning.X_train\n",
    "X_test = learning.X_test\n",
    "y_train = learning.y_train\n",
    "y_test = learning.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "model_test = LSTM(num_features=11, \n",
    "             hidden_dim=64, \n",
    "             series_length = input_size,\n",
    "             batch_size=batch_size,\n",
    "             output_dim=1, \n",
    "             num_layers=2)\n",
    "\n",
    "# model_test = LSTM(num_features=11, \n",
    "#              hidden_dim=64, \n",
    "# #              series_length = input_size,\n",
    "#              batch_size=batch_size,\n",
    "#              output_dim=1, \n",
    "#              num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for a toy case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "# batch_size = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load('X_train.pt').permute(1, 0, 2)\n",
    "X_test = torch.load('X_test.pt').permute(1, 0, 2)\n",
    "y_train = torch.load('y_train.pt')\n",
    "y_test = torch.load('y_test.pt')\n",
    "\n",
    "# y_train = y_train + 10\n",
    "# y_test = y_test + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "# 80 series\n",
    "# 20 is the length of a series\n",
    "# 1 feature\n",
    "\n",
    "# X_train [number of series, length of series, feature number]\n",
    "# y_train [number of series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = TensorDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_toy = LSTM(num_features=1, \n",
    "             hidden_dim=64, \n",
    "             batch_size=batch_size,\n",
    "             output_dim=1, \n",
    "             num_layers=2, \n",
    "             series_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = learning.pred_list\n",
    "observed = learning.y_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predicted, label=\"Predicted\")\n",
    "plt.plot(observed, label=\"Observed\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_log, label=\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[[\"target\"]]\n",
    "# Taking t-1 to be the value for t\n",
    "df[\"persistance\"] = df.shift(1)\n",
    "df.dropna(inplace=True)\n",
    "# Calculating metrics for these columns\n",
    "MSE, MAE, MDE = evaluate(df, \"target\", \"persistance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              target  persistance\n",
      "date                             \n",
      "2006-09-14  0.021675    -0.001404\n",
      "2006-09-15  0.043012     0.021675\n",
      "2006-09-18  0.013450     0.043012\n",
      "2006-09-19  0.025225     0.013450\n",
      "2006-09-20  0.024843     0.025225\n",
      "              target  persistance\n",
      "date                             \n",
      "2019-06-24 -0.000589     0.004104\n",
      "2019-06-25 -0.026401    -0.000589\n",
      "2019-06-26 -0.010480    -0.026401\n",
      "2019-06-27 -0.011358    -0.010480\n",
      "2019-06-28 -0.015457    -0.011358\n"
     ]
    }
   ],
   "source": [
    "print(df[:5])\n",
    "print(df[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>persistance</th>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.017105</td>\n",
       "      <td>0.465585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MSE       MAE       MDE\n",
       "Name                                     \n",
       "persistance  0.000582  0.017105  0.465585"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placing in results dataframe\n",
    "results = pd.DataFrame(columns={\"MSE\", \"MAE\", \"MDE\"})\n",
    "results.index.name = 'Name'\n",
    "results.head()\n",
    "results.loc[\"persistance\"] = [MSE, MAE, MDE] \n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chai_Base.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
