{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67j8odKJJQL_"
   },
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd88JeQfrEzc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeplearning import *\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from preprocessing import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import *\n",
    "import warnings\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insuring that training is done on GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/\"\n",
    "universe_dict = universe_select(path, \"Cu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k8FIHASpHfv"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ki2dJCGNkOVE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included Instrument:\n",
      "cu_shfe\n",
      "cu_lme\n",
      "cu_comex_p\n",
      "cu_comex_s\n",
      "peso\n",
      "sol\n",
      "bdi\n",
      "ted\n",
      "vix\n",
      "skew\n",
      "gsci\n"
     ]
    }
   ],
   "source": [
    "# Renaming the columns to price\n",
    "universe_dict = price_rename(universe_dict)\n",
    "# Cleaning the dataset of any erroneous datapoints\n",
    "universe_dict = clean_dict_gen(universe_dict)\n",
    "# Making sure that all the points in the window have consistent lenght\n",
    "universe_dict = truncate_window_length(universe_dict)\n",
    "df_full = generate_dataset(universe_dict, lg_returns_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target column represents the log returns at one forecast length out in the future for the instrument of interest (aluminium or copper prices on the London Metals Exchange). \n",
    "\n",
    "To normalise the independent variables, the 1 day log returns between closing prices have been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise the plots\n",
    "# visualise_universe(universe_dict)\n",
    "df = df_full[[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cu_shfe</th>\n",
       "      <th>cu_lme</th>\n",
       "      <th>cu_comex_p</th>\n",
       "      <th>cu_comex_s</th>\n",
       "      <th>peso</th>\n",
       "      <th>sol</th>\n",
       "      <th>bdi</th>\n",
       "      <th>ted</th>\n",
       "      <th>vix</th>\n",
       "      <th>skew</th>\n",
       "      <th>gsci</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-09-13</th>\n",
       "      <td>0.001583</td>\n",
       "      <td>-0.013602</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.055554</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>-0.004276</td>\n",
       "      <td>-0.064091</td>\n",
       "      <td>-0.048458</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>-0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-14</th>\n",
       "      <td>0.025695</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-0.104881</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>-0.001692</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.032559</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>0.021675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-15</th>\n",
       "      <td>-0.027710</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>-0.019557</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>0.124060</td>\n",
       "      <td>-0.004952</td>\n",
       "      <td>0.043012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-18</th>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>0.030146</td>\n",
       "      <td>0.032495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>-0.060186</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.013450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-19</th>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.012512</td>\n",
       "      <td>0.138083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>-0.018385</td>\n",
       "      <td>-0.019642</td>\n",
       "      <td>0.025225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cu_shfe    cu_lme  cu_comex_p  cu_comex_s      peso       sol  \\\n",
       "date                                                                         \n",
       "2006-09-13  0.001583 -0.013602    0.000590    0.055554 -0.000558  0.000307   \n",
       "2006-09-14  0.025695 -0.006098   -0.003100   -0.104881  0.000707 -0.001692   \n",
       "2006-09-15 -0.027710 -0.021468   -0.019557    0.041104 -0.001172  0.000154   \n",
       "2006-09-18  0.006177  0.024153    0.030146    0.032495  0.000000 -0.000924   \n",
       "2006-09-19  0.023214  0.007613   -0.012512    0.138083  0.000000  0.000000   \n",
       "\n",
       "                 bdi       ted       vix      skew      gsci    target  \n",
       "date                                                                    \n",
       "2006-09-13  0.029244 -0.004276 -0.064091 -0.048458  0.002624 -0.001404  \n",
       "2006-09-14  0.018715  0.005088  0.032559 -0.008086 -0.010694  0.021675  \n",
       "2006-09-15  0.012050  0.002636  0.018019  0.124060 -0.004952  0.043012  \n",
       "2006-09-18  0.004920  0.002023  0.001699 -0.060186  0.007824  0.013450  \n",
       "2006-09-19 -0.000935 -0.000202  0.016835 -0.018385 -0.019642  0.025225  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head(5)\n",
    "# df_full.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise data\n",
    "\n",
    "Split into validation data and test data\n",
    "\n",
    "Use validation to tune hyperparameters\n",
    "\n",
    "Perform predictions on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_full.loc[:, df_full.columns != 'target']\n",
    "df_y = df_full.loc[:, df_full.columns == 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearning():\n",
    "    \"\"\"Class to perform training and validation for a given model\"\"\"\n",
    "    def __init__(self, df_X, df_y,\n",
    "                 #model, \n",
    "                 #optimiser, \n",
    "                 loss_function=torch.nn.MSELoss(size_average=False),\n",
    "                 device=\"cpu\", seed=42):\n",
    "        \n",
    "        self.df_X = df_X\n",
    "        self.df_y = df_y\n",
    "   \n",
    "        #self.model = model\n",
    "        #self.optimiser = optimiser\n",
    "        self.loss_function = loss_function\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        \n",
    "        assert (type(self.df_X) == pd.DataFrame)\n",
    "        assert (type(self.df_y) == pd.DataFrame)\n",
    "        assert (len(self.df_X.index) == len(self.df_y.index))\n",
    "        assert (len(self.df_X.index) > 0)\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        \n",
    "    def train_val_test(self):\n",
    "        \"\"\"Splits the dataframes in to a training, validation\n",
    "        and test set and creates torch tensors from the underlying\n",
    "        numpy arrays\"\"\"\n",
    "        # Splitting the sets into train, test and validation\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.df_X, self.df_y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X_train, self.y_train, test_size=0.25, shuffle=False)\n",
    "\n",
    "        print(\"Train Length: \\t\\t%i\\nValidation Length: \\t%i\\nTest Length:\\t\\t%i\" % (len(X_train), len(X_val), len(X_test)))\n",
    "        \n",
    "        # Tensor of training data\n",
    "        self.X_train = torch.from_numpy(self.X_train.values).float()\n",
    "        self.y_train = torch.from_numpy(self.y_train.values).float()\n",
    "\n",
    "        # Tensor of training labels\n",
    "        self.X_val = torch.from_numpy(self.X_val.values).float()\n",
    "        self.y_val = torch.from_numpy(self.y_val.values).float()\n",
    "\n",
    "        #  Tensor of test data\n",
    "        self.X_test = torch.from_numpy(self.X_test.values).float()\n",
    "        self.y_test = torch.from_numpy(self.y_test.values).float()\n",
    "\n",
    "        # Size Check\n",
    "        print(\"\\nX Train Shape:\\t\\t\", self.X_train.size())\n",
    "        print(\"X Val Shape:\\t\\t\", self.X_val.size())\n",
    "        print(\"X Test Shape:\\t\\t\", self.X_test.size())\n",
    "\n",
    "        print(\"\\ny Train Shape:\\t\\t\", self.y_train.size())\n",
    "        print(\"y Val Shape:\\t\\t\", self.y_val.size())\n",
    "        print(\"y Test Shape:\\t\\t\", self.y_test.size())\n",
    "    \n",
    "    \n",
    "    def normalise(self):\n",
    "        \"\"\"Normalizes the data using MaxMinScaler, which\n",
    "            was chosen because it preserves the original \n",
    "            scale and doesn't reduce effect of outliers\"\"\"\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # Normalize the validation and test set by the same\n",
    "        # scale as the training data. Needed for training to\n",
    "        # be correctly scaled\n",
    "        \n",
    "        self.X_train = scaler.fit_transform(self.X_train)\n",
    "        self.X_val = scaler.transform(self.X_val)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "        \n",
    "        \"\"\"TODO implement output scaling for MTL because \n",
    "        otherwise the scale of the different outputs will\n",
    "        lead to dominance of some tasks\"\"\"\n",
    "        \n",
    "    \n",
    "    def create_data_loaders(self):\n",
    "        \"\"\"Forms iterators to pipeline in the data\"\"\"\n",
    "        \n",
    "        # Create tensor datasets\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        \n",
    "        # Data loaders\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # Sets the model to train mode\n",
    "        self.model.train()\n",
    "        \n",
    "        train_loss, train_accuracy = 0., 0.\n",
    "\n",
    "        # The data loader creates batches of data to train\n",
    "        for X_train_batch, y_train_batch in train_data_loader:\n",
    "\n",
    "        X_train_batch = X_train_batch.to(self.device)\n",
    "        y_train_batch = y_train_batch.to(self.device)\n",
    "\n",
    "        # Zeros the gradients\n",
    "        self.optimiser.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        y_pred = self.model(X_train_batch)\n",
    "\n",
    "        # Calculate loss for the batch\n",
    "        loss = self.loss_function(y_pred, y_train_batch)   \n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()   \n",
    "        \n",
    "        # Calculate the training loss\n",
    "        train_loss += (loss * X_train_batch.size()[0]).detach().cpu().numpy()\n",
    "        \n",
    "        \"\"\"TODO Put in an accuracy function as well\"\"\"\n",
    "        \n",
    "        # Update Parameters\n",
    "        self.optimiser.step()               \n",
    "        \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length: \t\t1999\n",
      "Validation Length: \t667\n",
      "Test Length:\t\t667\n",
      "\n",
      "X Train Shape:\t\t torch.Size([1999, 11])\n",
      "X Val Shape:\t\t torch.Size([667, 11])\n",
      "X Test Shape:\t\t torch.Size([667, 11])\n",
      "\n",
      "y Train Shape:\t\t torch.Size([1999, 1])\n",
      "y Val Shape:\t\t torch.Size([667, 1])\n",
      "y Test Shape:\t\t torch.Size([667, 1])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "momentum = 0.5\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "n_epochs = 2\n",
    "weight_decay=0\n",
    "\n",
    "df_X = df_full.loc[:, df_full.columns != 'target']\n",
    "df_y = df_full.loc[:, df_full.columns == 'target']\n",
    "\n",
    "# optimiser = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\"\"\"Do I need to declare a data explicitly in the dataset\"\"\"\n",
    "\n",
    "learning = DeepLearning(df_X, df_y)\n",
    "# Splitting the data into the train, validation and test sets\n",
    "learning.train_val_test()\n",
    "# Scaling the data by the training dataset\n",
    "learning.normalise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMnet(nn.Module):\n",
    "    \"\"\"A Long Short Term Memory network\n",
    "    model\"\"\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(LSTMnet, self, num_features, hidden_dim, num_layers, output_dim,\n",
    "              batch_size, debug=True).__init__()\n",
    "        \n",
    "        # Number of features\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # The output dimensions\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Batch Size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Dropout rate\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Debug Mode\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = self.num_features, \n",
    "            hidden_size =self.hidden_dim,\n",
    "            num_layers =self.num_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "    # This initialised the hidden state to be zeros\n",
    "    return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))\n",
    "    \n",
    "    \n",
    "    def forward(self, x)\n",
    "        \"\"\"Forward pass through the neural network\"\"\"\n",
    "        # Initialises the hidden states\n",
    "        h0, c0 = self.init_hidden()\n",
    "        # Passes through the lstm layer with hidden states\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # Make prediction from linear layer\n",
    "        y_pred = self.linear(out[-1].view(self.batch_size, -1))\n",
    "        \n",
    "        return y_pred.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(x / 1.0 / T).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[[\"target\"]]\n",
    "# Taking t-1 to be the value for t\n",
    "df[\"persistance\"] = df.shift(1)\n",
    "df.dropna(inplace=True)\n",
    "# Calculating metrics for these columns\n",
    "MSE, MAE, MDE = evaluate(df, \"target\", \"persistance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              target  persistance\n",
      "date                             \n",
      "2006-09-14  0.021675    -0.001404\n",
      "2006-09-15  0.043012     0.021675\n",
      "2006-09-18  0.013450     0.043012\n",
      "2006-09-19  0.025225     0.013450\n",
      "2006-09-20  0.024843     0.025225\n",
      "              target  persistance\n",
      "date                             \n",
      "2019-06-24 -0.000589     0.004104\n",
      "2019-06-25 -0.026401    -0.000589\n",
      "2019-06-26 -0.010480    -0.026401\n",
      "2019-06-27 -0.011358    -0.010480\n",
      "2019-06-28 -0.015457    -0.011358\n"
     ]
    }
   ],
   "source": [
    "print(df[:5])\n",
    "print(df[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>persistance</th>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.017105</td>\n",
       "      <td>0.465585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MSE       MAE       MDE\n",
       "Name                                     \n",
       "persistance  0.000582  0.017105  0.465585"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placing in results dataframe\n",
    "results = pd.DataFrame(columns={\"MSE\", \"MAE\", \"MDE\"})\n",
    "results.index.name = 'Name'\n",
    "results.head()\n",
    "results.loc[\"persistance\"] = [MSE, MAE, MDE] \n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chai_Base.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
